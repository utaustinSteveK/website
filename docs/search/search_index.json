{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"NetIntel-OCR","text":""},{"location":"#building-semantic-cmdb-with-ai-powered-document-intelligence","title":"Building Semantic CMDB with AI-Powered Document Intelligence","text":"<p>NetIntel-OCR transforms unstructured technical documentation into structured, searchable knowledge for building Semantic Configuration Management Databases (CMDB).</p>"},{"location":"#project-goal","title":"Project Goal","text":"<p>Create an intelligent document processing platform that extracts network architectures, security configurations, and operational workflows from enterprise documentation to populate a Semantic CMDB.</p>"},{"location":"#key-use-cases","title":"Key Use Cases","text":""},{"location":"#network-domain","title":"Network Domain","text":"<ul> <li>Architecture Discovery: Extract network topologies from design documents</li> <li>Configuration Mapping: Parse firewall rules and routing configurations</li> <li>Dependency Analysis: Identify component relationships and data flows</li> <li>Change Impact: Track architecture evolution across document versions</li> </ul>"},{"location":"#security-domain","title":"Security Domain","text":"<ul> <li>Zone Identification: Detect DMZ, trust boundaries, and security zones</li> <li>Compliance Mapping: Extract security controls and policy implementations</li> <li>Risk Assessment: Identify exposed services and attack surfaces</li> <li>Audit Trail: Document security architecture decisions and rationale</li> </ul>"},{"location":"#core-features","title":"Core Features","text":""},{"location":"#intelligent-detection","title":"\ud83c\udfaf Intelligent Detection","text":"<ul> <li>Network diagram recognition with 90%+ accuracy</li> <li>Flow chart and process diagram extraction</li> <li>Multi-diagram page processing</li> <li>Context-aware interpretation using surrounding text</li> </ul>"},{"location":"#mermaid-generation","title":"\ud83d\udd04 Mermaid Generation","text":"<ul> <li>Automatic conversion to Mermaid.js syntax</li> <li>Syntax validation and auto-correction</li> <li>Support for complex network topologies</li> <li>Preserves component relationships</li> </ul>"},{"location":"#context-extraction","title":"\ud83e\udde0 Context Extraction","text":"<ul> <li>Analyzes diagrams with document context</li> <li>Identifies critical components and data flows</li> <li>Provides security analysis and recommendations</li> <li>Generates architecture summaries</li> </ul>"},{"location":"#vector-search","title":"\ud83d\udd0d Vector Search","text":"<ul> <li>Semantic search across processed documents</li> <li>Milvus integration for scalable retrieval</li> <li>Component and relationship queries</li> <li>Cross-document knowledge linking</li> </ul>"},{"location":"#architecture","title":"Architecture","text":"<pre><code>graph TB\n    PDF[PDF Documents] --&gt; OCR[OCR Engine]\n    OCR --&gt; Detection[Diagram Detection]\n    Detection --&gt; Network[Network Processor]\n    Detection --&gt; Flow[Flow Processor]\n    Network --&gt; Mermaid[Mermaid Generator]\n    Flow --&gt; Mermaid\n    Mermaid --&gt; Context[Context Extractor]\n    Context --&gt; Vector[Vector Store]\n    Vector --&gt; CMDB[Semantic CMDB]</code></pre>"},{"location":"#getting-started","title":"Getting Started","text":"<p>NetIntel-OCR provides a CLI interface for processing technical documentation:</p> <pre><code># Process network architecture document\nnetintel-ocr --model Nanonets-OCR-s:latest \\\n             --network-model qwen2.5vl:7b \\\n             --debug cisco-sdwan-design-guide.pdf \\\n             --start 5 --end 10\n</code></pre> <p>See the Quick Start Guide for installation and basic usage.</p>"},{"location":"#documentation","title":"Documentation","text":"<ul> <li>Quick Start Guide - Installation and first steps</li> <li>Deployment Guide - Docker and Kubernetes setup</li> <li>Customization Guide - Prompt engineering and tuning</li> <li>Vector Search Guide - Milvus integration and queries</li> </ul>"},{"location":"#requirements","title":"Requirements","text":"<ul> <li>Python 3.10+</li> <li>Ollama for LLM inference</li> <li>8GB+ RAM for processing</li> <li>GPU recommended for faster inference</li> </ul>"},{"location":"#resources","title":"Resources","text":"<ul> <li>Documentation: https://visionml.net/docs</li> <li>PyPI Package: https://pypi.org/project/netintel-ocr/</li> <li>GitHub: https://github.com/VisionMLNet/NetIntelOCR</li> <li>Discord Community: https://discord.gg/netintel-ocr</li> </ul>"},{"location":"#license","title":"License","text":"<p>Enterprise license required for production use. Contact for pricing.</p>"},{"location":"api/","title":"API Integration Guide","text":""},{"location":"api/#rest-api-server","title":"REST API Server","text":"<p>NetIntel-OCR provides a REST API for programmatic document processing and integration with external systems.</p>"},{"location":"api/#starting-the-api-server","title":"Starting the API Server","text":"<pre><code># Start API server on port 8000\nnetintel-ocr --api\n\n# Custom port and host\nnetintel-ocr --api --port 8080 --host 0.0.0.0\n\n# With authentication\nnetintel-ocr --api --api-key YOUR_SECRET_KEY\n</code></pre>"},{"location":"api/#docker-api-mode","title":"Docker API Mode","text":"<pre><code>docker run -p 8000:8000 \\\n  -e API_KEY=your-secret-key \\\n  netintel-ocr:latest --api\n</code></pre>"},{"location":"api/#api-endpoints","title":"API Endpoints","text":""},{"location":"api/#health-check","title":"Health Check","text":"<pre><code>GET /health\n\nResponse:\n{\n  \"status\": \"healthy\",\n  \"version\": \"0.1.16.15\",\n  \"models_available\": [\"qwen2.5vl:7b\", \"Nanonets-OCR-s:latest\"],\n  \"milvus_connected\": true\n}\n</code></pre>"},{"location":"api/#process-document","title":"Process Document","text":"<pre><code>POST /process\nContent-Type: multipart/form-data\n\nParameters:\n- file: PDF file (required)\n- model: OCR model (optional)\n- network_model: Network diagram model (optional)\n- start_page: Starting page (optional)\n- end_page: Ending page (optional)\n- confidence_threshold: Detection threshold (optional)\n\nResponse:\n{\n  \"job_id\": \"uuid-12345\",\n  \"status\": \"processing\",\n  \"estimated_time\": 45\n}\n</code></pre>"},{"location":"api/#get-job-status","title":"Get Job Status","text":"<pre><code>GET /status/{job_id}\n\nResponse:\n{\n  \"job_id\": \"uuid-12345\",\n  \"status\": \"completed\",\n  \"progress\": 100,\n  \"pages_processed\": 10,\n  \"diagrams_found\": 3\n}\n</code></pre>"},{"location":"api/#get-results","title":"Get Results","text":"<pre><code>GET /results/{job_id}\n\nResponse:\n{\n  \"job_id\": \"uuid-12345\",\n  \"pages\": [\n    {\n      \"page_number\": 1,\n      \"type\": \"text\",\n      \"content\": \"...\"\n    },\n    {\n      \"page_number\": 2,\n      \"type\": \"network_diagram\",\n      \"mermaid\": \"graph TB...\",\n      \"components\": [...],\n      \"context\": {...}\n    }\n  ],\n  \"summary\": {...}\n}\n</code></pre>"},{"location":"api/#search-documents","title":"Search Documents","text":"<pre><code>POST /search\nContent-Type: application/json\n\n{\n  \"query\": \"firewall configuration\",\n  \"collection\": \"network_docs\",\n  \"limit\": 10,\n  \"filters\": {\n    \"document_type\": \"network\"\n  }\n}\n\nResponse:\n{\n  \"results\": [\n    {\n      \"document\": \"firewall-guide.pdf\",\n      \"page\": 5,\n      \"score\": 0.92,\n      \"content\": \"...\",\n      \"metadata\": {...}\n    }\n  ]\n}\n</code></pre>"},{"location":"api/#python-client","title":"Python Client","text":""},{"location":"api/#installation","title":"Installation","text":"<pre><code># Install client library\npip install netintel-ocr-client\n\n# Or install full package with client\npip install netintel-ocr[client]\n</code></pre> <p>Package Details</p> <p>Main package: https://pypi.org/project/netintel-ocr/</p> <p>Client library: https://pypi.org/project/netintel-ocr-client/</p>"},{"location":"api/#basic-usage","title":"Basic Usage","text":"<pre><code>from netintel_client import NetIntelClient\n\n# Initialize client\nclient = NetIntelClient(\n    host=\"http://localhost:8000\",\n    api_key=\"your-secret-key\"\n)\n\n# Process document\njob = client.process_document(\n    file_path=\"network-design.pdf\",\n    model=\"qwen2.5vl:7b\",\n    start_page=1,\n    end_page=10\n)\n\n# Wait for completion\nresult = client.wait_for_job(job.job_id)\n\n# Get results\npages = result.pages\ndiagrams = [p for p in pages if p.type == \"network_diagram\"]\n</code></pre>"},{"location":"api/#async-processing","title":"Async Processing","text":"<pre><code>import asyncio\nfrom netintel_client import AsyncNetIntelClient\n\nasync def process_documents():\n    client = AsyncNetIntelClient(\"http://localhost:8000\")\n\n    # Process multiple documents\n    jobs = []\n    for pdf in pdf_files:\n        job = await client.process_document(pdf)\n        jobs.append(job)\n\n    # Wait for all\n    results = await asyncio.gather(\n        *[client.wait_for_job(j.job_id) for j in jobs]\n    )\n\n    return results\n</code></pre>"},{"location":"api/#javascripttypescript-client","title":"JavaScript/TypeScript Client","text":""},{"location":"api/#installation_1","title":"Installation","text":"<pre><code>npm install netintel-ocr-client\n</code></pre>"},{"location":"api/#usage","title":"Usage","text":"<pre><code>const { NetIntelClient } = require('netintel-ocr-client');\n\nconst client = new NetIntelClient({\n  host: 'http://localhost:8000',\n  apiKey: 'your-secret-key'\n});\n\n// Process document\nconst job = await client.processDocument({\n  file: fileBuffer,\n  model: 'qwen2.5vl:7b'\n});\n\n// Get results\nconst result = await client.waitForJob(job.jobId);\nconsole.log(`Found ${result.diagramsFound} diagrams`);\n</code></pre>"},{"location":"api/#webhook-integration","title":"Webhook Integration","text":""},{"location":"api/#configure-webhooks","title":"Configure Webhooks","text":"<pre><code>POST /webhooks\nContent-Type: application/json\n\n{\n  \"url\": \"https://your-server.com/webhook\",\n  \"events\": [\"job.completed\", \"job.failed\"],\n  \"secret\": \"webhook-secret\"\n}\n</code></pre>"},{"location":"api/#webhook-payload","title":"Webhook Payload","text":"<pre><code>{\n  \"event\": \"job.completed\",\n  \"job_id\": \"uuid-12345\",\n  \"timestamp\": \"2024-01-15T10:30:00Z\",\n  \"data\": {\n    \"pages_processed\": 10,\n    \"diagrams_found\": 3,\n    \"processing_time\": 45.2\n  }\n}\n</code></pre>"},{"location":"api/#rate-limiting","title":"Rate Limiting","text":"<p>Default limits: - 100 requests per minute per API key - 10 concurrent jobs per API key - 100MB max file size</p> <p>Configure custom limits:</p> <pre><code>netintel-ocr --api \\\n  --rate-limit 200 \\\n  --concurrent-jobs 20 \\\n  --max-file-size 500\n</code></pre>"},{"location":"api/#authentication","title":"Authentication","text":""},{"location":"api/#api-key-authentication","title":"API Key Authentication","text":"<pre><code># Set API key\nexport NETINTEL_API_KEY=your-secret-key\n\n# Or in request header\ncurl -H \"X-API-Key: your-secret-key\" \\\n  http://localhost:8000/process\n</code></pre>"},{"location":"api/#jwt-authentication","title":"JWT Authentication","text":"<pre><code># Get token\nPOST /auth/token\n{\n  \"username\": \"user\",\n  \"password\": \"pass\"\n}\n\n# Use token\ncurl -H \"Authorization: Bearer jwt-token\" \\\n  http://localhost:8000/process\n</code></pre>"},{"location":"api/#error-handling","title":"Error Handling","text":""},{"location":"api/#error-response-format","title":"Error Response Format","text":"<pre><code>{\n  \"error\": {\n    \"code\": \"INVALID_MODEL\",\n    \"message\": \"Model 'unknown-model' not found\",\n    \"details\": {\n      \"available_models\": [\"qwen2.5vl:7b\", \"llava:13b\"]\n    }\n  },\n  \"request_id\": \"req-12345\"\n}\n</code></pre>"},{"location":"api/#common-error-codes","title":"Common Error Codes","text":"Code Description Solution <code>INVALID_FILE</code> PDF file corrupt or invalid Verify PDF file <code>MODEL_NOT_FOUND</code> Requested model unavailable Check available models <code>RATE_LIMITED</code> Too many requests Retry after delay <code>PROCESSING_FAILED</code> Internal processing error Check logs <code>TIMEOUT</code> Processing timeout Reduce page range"},{"location":"api/#monitoring","title":"Monitoring","text":""},{"location":"api/#metrics-endpoint","title":"Metrics Endpoint","text":"<pre><code>GET /metrics\n\nResponse (Prometheus format):\nnetintel_requests_total{method=\"POST\",endpoint=\"/process\"} 1234\nnetintel_processing_duration_seconds{quantile=\"0.99\"} 45.2\nnetintel_active_jobs 5\n</code></pre>"},{"location":"api/#logging","title":"Logging","text":"<pre><code># Enable debug logging\nclient = NetIntelClient(\n    host=\"http://localhost:8000\",\n    log_level=\"DEBUG\"\n)\n</code></pre>"},{"location":"api/#next-steps","title":"Next Steps","text":"<ul> <li>MCP Server Guide - Model Context Protocol integration</li> <li>Batch Processing - Process multiple documents</li> <li>Deployment Guide - Production setup</li> </ul>"},{"location":"batch/","title":"Batch Processing Guide","text":""},{"location":"batch/#overview","title":"Overview","text":"<p>NetIntel-OCR provides efficient batch processing capabilities for handling multiple documents with parallel processing, progress tracking, and centralized storage.</p>"},{"location":"batch/#basic-batch-processing","title":"Basic Batch Processing","text":""},{"location":"batch/#process-multiple-files","title":"Process Multiple Files","text":"<pre><code># Process all PDFs in directory\nnetintel-ocr --batch *.pdf\n\n# With specific model\nnetintel-ocr --batch --model qwen2.5vl:7b *.pdf\n\n# Recursive processing\nnetintel-ocr --batch --recursive /path/to/documents\n</code></pre>"},{"location":"batch/#parallel-processing","title":"Parallel Processing","text":"<pre><code># Process 4 documents simultaneously\nnetintel-ocr --batch --max-parallel 4 *.pdf\n\n# Auto-detect optimal parallelism\nnetintel-ocr --batch --auto-parallel *.pdf\n\n# GPU parallel processing\nnetintel-ocr --batch --gpu --max-parallel 2 *.pdf\n</code></pre>"},{"location":"batch/#advanced-batch-features","title":"Advanced Batch Features","text":""},{"location":"batch/#batch-ingestion","title":"Batch Ingestion","text":"<pre><code># Ingest to vector store\nnetintel-ocr --batch-ingest \\\n  --collection network_docs \\\n  --max-parallel 4 \\\n  *.pdf\n\n# With deduplication\nnetintel-ocr --batch-ingest \\\n  --deduplicate \\\n  --collection unified_docs \\\n  /path/to/documents/*.pdf\n</code></pre>"},{"location":"batch/#progress-tracking","title":"Progress Tracking","text":"<pre><code># Enable progress bar\nnetintel-ocr --batch --progress *.pdf\n\n# Save progress for resume\nnetintel-ocr --batch \\\n  --checkpoint batch-checkpoint.json \\\n  --resume-on-failure \\\n  *.pdf\n</code></pre>"},{"location":"batch/#output-organization","title":"Output Organization","text":"<pre><code># Organize by document type\nnetintel-ocr --batch \\\n  --output-structure type \\\n  --output-dir processed/ \\\n  *.pdf\n\n# Result:\n# processed/\n#   \u251c\u2500\u2500 network_diagrams/\n#   \u251c\u2500\u2500 flow_diagrams/\n#   \u2514\u2500\u2500 text_only/\n</code></pre>"},{"location":"batch/#batch-configuration","title":"Batch Configuration","text":""},{"location":"batch/#yaml-configuration","title":"YAML Configuration","text":"<pre><code># batch-config.yaml\nbatch:\n  max_parallel: 4\n  chunk_size: 10\n  resume_on_failure: true\n  checkpoint_file: batch-state.json\n\n  output:\n    structure: document  # or 'type', 'date'\n    dir: ./processed\n\n  models:\n    text: Nanonets-OCR-s:latest\n    network: qwen2.5vl:7b\n    flow: qwen2.5vl:7b\n\n  filters:\n    min_pages: 5\n    max_pages: 500\n    file_types: [pdf, png, jpg]\n\n  error_handling:\n    max_retries: 3\n    retry_delay: 5\n    skip_on_error: false\n</code></pre>"},{"location":"batch/#use-configuration","title":"Use Configuration","text":"<pre><code>netintel-ocr --batch --config batch-config.yaml *.pdf\n</code></pre>"},{"location":"batch/#centralized-database","title":"Centralized Database","text":""},{"location":"batch/#merge-to-central-store","title":"Merge to Central Store","text":"<pre><code># Create centralized database\nnetintel-ocr --merge-to-centralized \\\n  --source-dir ./processed \\\n  --central-db ./central/unified.db\n\n# With metadata\nnetintel-ocr --merge-to-centralized \\\n  --add-metadata \"project=network-refresh\" \\\n  --add-metadata \"date=2024-01-15\" \\\n  ./processed/* ./central/unified.db\n</code></pre>"},{"location":"batch/#query-centralized-database","title":"Query Centralized Database","text":"<pre><code># Search across all documents\nnetintel-ocr --query \\\n  --db ./central/unified.db \\\n  \"firewall configuration\"\n\n# Filter by metadata\nnetintel-ocr --query \\\n  --db ./central/unified.db \\\n  --filter \"project=network-refresh\" \\\n  \"DMZ architecture\"\n</code></pre>"},{"location":"batch/#cloud-storage-integration","title":"Cloud Storage Integration","text":""},{"location":"batch/#s3minio-support","title":"S3/MinIO Support","text":"<pre><code># Configure S3\nexport AWS_ACCESS_KEY_ID=your-key\nexport AWS_SECRET_ACCESS_KEY=your-secret\nexport S3_BUCKET=netintel-output\n\n# Process and upload to S3\nnetintel-ocr --batch \\\n  --output-s3 s3://netintel-output/processed/ \\\n  *.pdf\n\n# Process from S3\nnetintel-ocr --batch \\\n  --input-s3 s3://netintel-input/*.pdf \\\n  --output-s3 s3://netintel-output/\n</code></pre>"},{"location":"batch/#azure-blob-storage","title":"Azure Blob Storage","text":"<pre><code># Configure Azure\nexport AZURE_STORAGE_CONNECTION_STRING=your-connection-string\n\n# Process with Azure storage\nnetintel-ocr --batch \\\n  --storage-backend azure \\\n  --container processed-docs \\\n  *.pdf\n</code></pre>"},{"location":"batch/#performance-optimization","title":"Performance Optimization","text":""},{"location":"batch/#memory-management","title":"Memory Management","text":"<pre><code># Limit memory per process\nnetintel-ocr --batch \\\n  --max-memory 4GB \\\n  --max-parallel 2 \\\n  *.pdf\n\n# Enable swap for large documents\nnetintel-ocr --batch \\\n  --enable-swap \\\n  --swap-dir /tmp/netintel-swap \\\n  large-docs/*.pdf\n</code></pre>"},{"location":"batch/#cpugpu-optimization","title":"CPU/GPU Optimization","text":"<pre><code># CPU-only batch processing\nnetintel-ocr --batch \\\n  --cpu-only \\\n  --max-parallel $(nproc) \\\n  *.pdf\n\n# Mixed CPU/GPU processing\nnetintel-ocr --batch \\\n  --gpu-for-models \"llava,qwen2.5vl\" \\\n  --cpu-for-models \"Nanonets-OCR-s\" \\\n  *.pdf\n</code></pre>"},{"location":"batch/#caching-strategy","title":"Caching Strategy","text":"<pre><code># Enable aggressive caching\nnetintel-ocr --batch \\\n  --cache-models \\\n  --cache-embeddings \\\n  --cache-dir /tmp/netintel-cache \\\n  *.pdf\n\n# Share cache across runs\nexport NETINTEL_CACHE_DIR=/shared/cache\nnetintel-ocr --batch --use-cache *.pdf\n</code></pre>"},{"location":"batch/#monitoring-and-logging","title":"Monitoring and Logging","text":""},{"location":"batch/#real-time-monitoring","title":"Real-time Monitoring","text":"<pre><code># Enable metrics server\nnetintel-ocr --batch \\\n  --metrics-port 9090 \\\n  --progress-webhook http://monitor/progress \\\n  *.pdf\n\n# View metrics\ncurl http://localhost:9090/metrics\n</code></pre>"},{"location":"batch/#detailed-logging","title":"Detailed Logging","text":"<pre><code># Per-document logs\nnetintel-ocr --batch \\\n  --log-level DEBUG \\\n  --log-per-document \\\n  --log-dir ./logs \\\n  *.pdf\n\n# Structured logging\nnetintel-ocr --batch \\\n  --log-format json \\\n  --log-file batch.jsonl \\\n  *.pdf\n</code></pre>"},{"location":"batch/#error-handling","title":"Error Handling","text":""},{"location":"batch/#retry-logic","title":"Retry Logic","text":"<pre><code># Automatic retry with backoff\nnetintel-ocr --batch \\\n  --max-retries 3 \\\n  --retry-backoff exponential \\\n  --retry-delay 5 \\\n  *.pdf\n</code></pre>"},{"location":"batch/#failed-document-handling","title":"Failed Document Handling","text":"<pre><code># Skip failed documents\nnetintel-ocr --batch \\\n  --skip-on-error \\\n  --failed-list failed.txt \\\n  *.pdf\n\n# Reprocess failed documents\nnetintel-ocr --batch \\\n  --input-list failed.txt \\\n  --max-retries 5\n</code></pre>"},{"location":"batch/#batch-scripts","title":"Batch Scripts","text":""},{"location":"batch/#shell-script-example","title":"Shell Script Example","text":"<pre><code>#!/bin/bash\n# batch-process.sh\n\nDOCS_DIR=\"/path/to/documents\"\nOUTPUT_DIR=\"/path/to/output\"\nFAILED_LIST=\"failed_docs.txt\"\n\n# Clear previous failures\n&gt; $FAILED_LIST\n\n# Process in chunks\nfind $DOCS_DIR -name \"*.pdf\" | while read -r file; do\n  netintel-ocr \\\n    --model qwen2.5vl:7b \\\n    --output-dir $OUTPUT_DIR \\\n    \"$file\" || echo \"$file\" &gt;&gt; $FAILED_LIST\ndone\n\n# Retry failed documents\nif [ -s $FAILED_LIST ]; then\n  echo \"Retrying failed documents...\"\n  while read -r file; do\n    netintel-ocr --model minicpm-v:latest \"$file\"\n  done &lt; $FAILED_LIST\nfi\n</code></pre>"},{"location":"batch/#python-script-example","title":"Python Script Example","text":"<pre><code># batch_processor.py\nimport os\nfrom pathlib import Path\nfrom netintel_ocr import BatchProcessor\n\nprocessor = BatchProcessor(\n    max_parallel=4,\n    model=\"qwen2.5vl:7b\",\n    output_dir=\"./processed\"\n)\n\n# Process all PDFs\npdf_files = Path(\"/documents\").glob(\"**/*.pdf\")\nresults = processor.process_batch(pdf_files)\n\n# Handle results\nfor result in results:\n    if result.success:\n        print(f\"\u2713 {result.file}: {result.diagrams_found} diagrams\")\n    else:\n        print(f\"\u2717 {result.file}: {result.error}\")\n\n# Generate summary\nprocessor.generate_summary(\"batch_summary.json\")\n</code></pre>"},{"location":"batch/#best-practices","title":"Best Practices","text":"<ol> <li>Chunk Large Batches: Process in groups of 50-100 documents</li> <li>Use Checkpoints: Enable resume for long-running batches</li> <li>Monitor Memory: Set limits to prevent OOM errors</li> <li>Deduplicate First: Remove duplicates before processing</li> <li>Test Small Sample: Validate settings on subset first</li> </ol>"},{"location":"batch/#next-steps","title":"Next Steps","text":"<ul> <li>Troubleshooting - Common batch issues</li> <li>Vector Search - Search processed batches</li> <li>API Integration - Batch processing via API</li> </ul>"},{"location":"changelog/","title":"Changelog","text":""},{"location":"changelog/#version-011615-latest-release","title":"Version 0.1.16.15 - Latest Release","text":"<p>Released: 2025-09-01</p>"},{"location":"changelog/#bug-fixes","title":"\ud83d\udc1b Bug Fixes","text":"<ul> <li>Fixed DEFAULT token parser error by replacing 'default' with 'DefaultZone'</li> <li>Enhanced connection handling to replace 'default' in arrow connections</li> <li>Improved keyword conflict resolution for Mermaid reserved words</li> </ul>"},{"location":"changelog/#version-011614","title":"Version 0.1.16.14","text":"<p>Released: 2025-09-01</p>"},{"location":"changelog/#bug-fixes_1","title":"\ud83d\udc1b Bug Fixes","text":"<ul> <li>Fixed flow diagram parse errors with node-subgraph concatenation patterns</li> <li>Enhanced Mermaid fixer to handle 'default' keyword issues in subgraphs</li> <li>Improved preprocessing to separate concatenated node and subgraph definitions</li> </ul>"},{"location":"changelog/#version-011613","title":"Version 0.1.16.13","text":"<p>Released: 2025-09-01</p>"},{"location":"changelog/#features","title":"\u2728 Features","text":"<ul> <li>Applied Mermaid validation fixes to flow diagrams</li> <li>Added context extraction to flow diagrams using surrounding text</li> <li>Enhanced flow processor with RobustMermaidValidator for auto-correction</li> </ul>"},{"location":"changelog/#version-011612","title":"Version 0.1.16.12","text":"<p>Released: 2025-09-01</p>"},{"location":"changelog/#features_1","title":"\u2728 Features","text":"<ul> <li>Added context extraction for diagrams using surrounding text paragraphs</li> <li>Enhanced validation to auto-correct LLM-generated Mermaid syntax issues</li> </ul>"},{"location":"changelog/#bug-fixes_2","title":"\ud83d\udc1b Bug Fixes","text":"<ul> <li>Fixed Mermaid diagram parsing errors with malformed subgraph/zone syntax</li> </ul>"},{"location":"changelog/#version-0116-major-release","title":"Version 0.1.16 - Major Release","text":"<p>Released: 2025-08-31</p>"},{"location":"changelog/#major-features","title":"\ud83c\udfaf Major Features","text":"<ul> <li>Unified Diagram Detection: Automatic detection for network/flow/hybrid diagrams</li> <li>Comprehensive Flow Processing: Full Mermaid generation for flow diagrams</li> <li>Context-Aware Analysis: Uses surrounding text (2 paragraphs before/after)</li> <li>Prompt Management System: Full customization without code changes</li> <li>Default Model Update: NetIntelOCR-7B-0925 as default vision model</li> </ul>"},{"location":"changelog/#new-capabilities","title":"\u2728 New Capabilities","text":"<ul> <li>Flow diagram element extraction and Mermaid generation</li> <li>Context extraction using document surrounding text</li> <li>Complete prompt import/export system</li> <li>Enhanced syntax validation and auto-correction</li> </ul>"},{"location":"changelog/#version-0115","title":"Version 0.1.15","text":"<p>Released: 2025-08-30</p>"},{"location":"changelog/#performance-improvements","title":"\ud83d\ude80 Performance Improvements","text":"<ul> <li>Milvus Integration: 20-60x faster search, 70% less memory usage</li> <li>Qwen3-8B Embeddings: 4096-dimensional vectors via Ollama</li> <li>Binary Vectors: Enhanced deduplication with SimHash</li> <li>Simplified Deployment: One-command initialization with scales</li> </ul>"},{"location":"changelog/#infrastructure","title":"\ud83d\udd27 Infrastructure","text":"<ul> <li>IVF_SQ8 indexing for CPU-optimized search</li> <li>Distributed architecture support</li> <li>Enhanced C++ deduplication core with AVX2 SIMD</li> </ul>"},{"location":"changelog/#version-0113","title":"Version 0.1.13","text":"<p>Released: 2025-08-25</p>"},{"location":"changelog/#new-features","title":"\u2728 New Features","text":"<ul> <li>REST API Server: Full API mode with <code>--api</code> flag</li> <li>MCP Server: Model Context Protocol support with <code>--mcp</code></li> <li>All-in-One Mode: Combined services with <code>--all-in-one</code></li> <li>Deployment Scales: Small/medium/large/enterprise configurations</li> <li>Kubernetes Support: Helm charts and manifests generation</li> </ul>"},{"location":"changelog/#version-0112","title":"Version 0.1.12","text":"<p>Released: 2025-08-20</p>"},{"location":"changelog/#major-features_1","title":"\ud83c\udfaf Major Features","text":"<ul> <li>Centralized Database: Unified LanceDB management</li> <li>Advanced Query Engine: Multi-field filtering and reranking</li> <li>Parallel Batch Processing: Progress tracking and resumability</li> <li>Cloud Storage: S3/MinIO integration</li> <li>Enhanced Embeddings: Multiple providers with intelligent caching</li> </ul>"},{"location":"changelog/#version-0110","title":"Version 0.1.10","text":"<p>Released: 2025-08-15</p>"},{"location":"changelog/#features_2","title":"\u2728 Features","text":"<ul> <li>Hybrid Detection: Automatic network/flow diagram classification</li> <li>Improved Accuracy: Enhanced component extraction algorithms</li> <li>Better Error Handling: Graceful fallbacks for processing failures</li> </ul>"},{"location":"changelog/#version-017","title":"Version 0.1.7","text":"<p>Released: 2025-08-10</p>"},{"location":"changelog/#major-features_2","title":"\ud83c\udfaf Major Features","text":"<ul> <li>Vector Database: Automatic LanceDB file generation</li> <li>RAG Optimization: Minimal metadata for optimal search</li> <li>Chunk Management: Intelligent document chunking</li> </ul>"},{"location":"changelog/#version-014","title":"Version 0.1.4","text":"<p>Released: 2025-08-01</p>"},{"location":"changelog/#new-features_1","title":"\u2728 New Features","text":"<ul> <li>Multi-Model Support: Different models for different tasks</li> <li>Model Optimization: Task-specific model selection</li> <li>Performance Modes: Fast/balanced/accurate processing</li> </ul>"},{"location":"changelog/#version-010","title":"Version 0.1.0","text":"<p>Released: 2025-07-01</p>"},{"location":"changelog/#initial-release","title":"\ud83c\udf89 Initial Release","text":"<ul> <li>Network diagram detection and extraction</li> <li>Mermaid.js generation</li> <li>PDF processing with OCR</li> <li>Basic CLI interface</li> <li>Ollama integration</li> </ul>"},{"location":"changelog/#upcoming-features","title":"Upcoming Features","text":""},{"location":"changelog/#version-020-planned","title":"Version 0.2.0 (Planned)","text":"<ul> <li>Web UI interface</li> <li>Real-time collaboration</li> <li>Custom model training</li> <li>Enterprise SSO integration</li> <li>Advanced analytics dashboard</li> </ul>"},{"location":"changelog/#version-030-planned","title":"Version 0.3.0 (Planned)","text":"<ul> <li>AutoML for model selection</li> <li>Federated learning support</li> <li>Multi-language support</li> <li>Graph database integration</li> <li>Compliance reporting</li> </ul>"},{"location":"changelog/#migration-guides","title":"Migration Guides","text":""},{"location":"changelog/#from-0115-to-0116","title":"From 0.1.15 to 0.1.16","text":"<ul> <li>Update default model to NetIntelOCR-7B-0925</li> <li>Export and update prompts using new management system</li> <li>Test flow diagram processing with new validator</li> </ul>"},{"location":"changelog/#from-0112-to-0115","title":"From 0.1.12 to 0.1.15","text":"<ul> <li>Migrate from LanceDB to Milvus</li> <li>Update embedding dimensions to 4096</li> <li>Regenerate vector indices</li> </ul>"},{"location":"changelog/#from-017-to-0112","title":"From 0.1.7 to 0.1.12","text":"<ul> <li>Update batch processing scripts</li> <li>Configure cloud storage backends</li> <li>Migrate to centralized database</li> </ul>"},{"location":"changelog/#deprecation-notices","title":"Deprecation Notices","text":""},{"location":"changelog/#deprecated-in-0116","title":"Deprecated in 0.1.16","text":"<ul> <li>Old flow diagram processor (use enhanced version)</li> <li>Manual prompt editing in code (use prompt management)</li> </ul>"},{"location":"changelog/#deprecated-in-0115","title":"Deprecated in 0.1.15","text":"<ul> <li>LanceDB backend (use Milvus)</li> <li>768-dimension embeddings (use 4096)</li> </ul>"},{"location":"changelog/#will-be-removed-in-020","title":"Will be removed in 0.2.0","text":"<ul> <li>Legacy CLI arguments</li> <li>Old configuration format</li> <li>Direct Ollama API calls</li> </ul>"},{"location":"changelog/#support","title":"Support","text":"<p>For issues and questions: - GitHub: https://github.com/VisionMLNet/NetIntelOCR/issues - Documentation: https://visionml.net/docs - PyPI: https://pypi.org/project/netintel-ocr/ - Discord: https://discord.gg/netintel-ocr</p>"},{"location":"customization/","title":"Customization Guide","text":""},{"location":"customization/#prompt-management","title":"Prompt Management","text":""},{"location":"customization/#export-current-prompts","title":"Export Current Prompts","text":"<pre><code># Export all prompts to YAML\nnetintel-ocr --export-prompts prompts.yaml\n\n# Export specific category\nnetintel-ocr --export-prompts network-prompts.yaml --category network\n</code></pre>"},{"location":"customization/#import-custom-prompts","title":"Import Custom Prompts","text":"<pre><code># Import modified prompts\nnetintel-ocr --import-prompts custom-prompts.yaml\n\n# Validate prompts\nnetintel-ocr --validate-prompts custom-prompts.yaml\n</code></pre>"},{"location":"customization/#prompt-structure","title":"Prompt Structure","text":"<pre><code>network_detection:\n  description: \"Detect network diagrams in images\"\n  prompt: |\n    Analyze this image for network architecture elements:\n    - Routers, switches, firewalls\n    - Network zones and segments\n    - Connection types and protocols\n\n    Return confidence score 0-1.\n\ncomponent_extraction:\n  description: \"Extract network components\"\n  prompt: |\n    Identify all network components:\n    {\n      \"components\": [...],\n      \"connections\": [...],\n      \"zones\": [...]\n    }\n</code></pre>"},{"location":"customization/#key-tuning-parameters","title":"Key Tuning Parameters","text":""},{"location":"customization/#1-detection-confidence-threshold","title":"1. Detection Confidence Threshold","text":"<pre><code># Default: 0.7\nnetintel-ocr --confidence-threshold 0.9 document.pdf\n\n# Lower for more detection (may include false positives)\nnetintel-ocr --confidence-threshold 0.5 document.pdf\n</code></pre>"},{"location":"customization/#2-model-temperature","title":"2. Model Temperature","text":"<pre><code># In prompts.yaml\nnetwork_extraction:\n  temperature: 0.3  # Lower = more deterministic\n  max_tokens: 4096\n  top_p: 0.9\n</code></pre>"},{"location":"customization/#3-context-window-size","title":"3. Context Window Size","text":"<pre><code># Surrounding text for context extraction\nnetintel-ocr --context-before 1000 --context-after 1000 document.pdf\n\n# Disable context for speed\nnetintel-ocr --no-context document.pdf\n</code></pre>"},{"location":"customization/#4-extraction-strategies","title":"4. Extraction Strategies","text":"<pre><code># Fast extraction (less accurate)\nnetintel-ocr --fast-extraction document.pdf\n\n# Comprehensive extraction (slower)\nnetintel-ocr --comprehensive document.pdf\n\n# Multi-pass extraction\nnetintel-ocr --multi-pass 3 document.pdf\n</code></pre>"},{"location":"customization/#5-output-verbosity","title":"5. Output Verbosity","text":"<pre><code># Minimal output\nnetintel-ocr --output minimal document.pdf\n\n# Include all metadata\nnetintel-ocr --output detailed document.pdf\n\n# Custom fields\nnetintel-ocr --include-fields \"components,security,recommendations\" document.pdf\n</code></pre>"},{"location":"customization/#6-mermaid-syntax-preferences","title":"6. Mermaid Syntax Preferences","text":"<pre><code># In config.yaml\nmermaid:\n  direction: TB  # Top-Bottom (or LR, RL, BT)\n  theme: default\n  node_shape: rectangle  # Or circle, diamond, hexagon\n  include_labels: true\n  simplify_connections: false\n</code></pre>"},{"location":"customization/#model-selection-tips","title":"Model Selection Tips","text":""},{"location":"customization/#vision-models-comparison","title":"Vision Models Comparison","text":"Model Speed Accuracy Memory Best For qwen2.5vl:7b Fast High 8GB General use llava:13b Medium Very High 16GB Complex diagrams minicpm-v Very Fast Medium 4GB Quick processing cogvlm Slow Highest 32GB Critical accuracy"},{"location":"customization/#selecting-models-by-document-type","title":"Selecting Models by Document Type","text":"<pre><code># Technical specifications\nnetintel-ocr --model Nanonets-OCR-s:latest \\\n             --network-model cogvlm:latest \\\n             technical-spec.pdf\n\n# Marketing materials (simpler diagrams)\nnetintel-ocr --model minicpm-v:latest \\\n             --network-model minicpm-v:latest \\\n             marketing-doc.pdf\n\n# Security documentation\nnetintel-ocr --model qwen2.5vl:7b \\\n             --security-focus \\\n             security-guide.pdf\n</code></pre>"},{"location":"customization/#custom-processing-pipelines","title":"Custom Processing Pipelines","text":""},{"location":"customization/#define-custom-pipeline","title":"Define Custom Pipeline","text":"<pre><code># custom_pipeline.py\nfrom netintel_ocr import Pipeline\n\npipeline = Pipeline()\npipeline.add_step(\"ocr\", model=\"Nanonets-OCR-s:latest\")\npipeline.add_step(\"detect\", confidence=0.8)\npipeline.add_step(\"extract\", strategy=\"comprehensive\")\npipeline.add_step(\"validate\", fix_errors=True)\npipeline.add_step(\"context\", window=2000)\n</code></pre>"},{"location":"customization/#use-custom-pipeline","title":"Use Custom Pipeline","text":"<pre><code>netintel-ocr --pipeline custom_pipeline.py document.pdf\n</code></pre>"},{"location":"customization/#advanced-prompt-engineering","title":"Advanced Prompt Engineering","text":""},{"location":"customization/#component-extraction-enhancement","title":"Component Extraction Enhancement","text":"<pre><code>component_extraction:\n  prompt: |\n    Extract network components with these specific details:\n    1. Component type (router/switch/firewall/server/endpoint)\n    2. Component name/label\n    3. IP addresses or network ranges\n    4. Security zone assignment\n    5. Criticality level (high/medium/low)\n\n    For each connection include:\n    - Source and destination components\n    - Protocol/port information\n    - Direction (unidirectional/bidirectional)\n    - Bandwidth or capacity if shown\n</code></pre>"},{"location":"customization/#security-analysis-focus","title":"Security Analysis Focus","text":"<pre><code>security_analysis:\n  prompt: |\n    Analyze security aspects:\n    - Identify trust boundaries and security zones\n    - Detect exposed services and entry points\n    - Map data flows crossing zone boundaries\n    - Identify potential attack vectors\n    - Suggest security improvements\n\n    Priority: Accuracy over speed\n</code></pre>"},{"location":"customization/#performance-optimization","title":"Performance Optimization","text":""},{"location":"customization/#batch-processing","title":"Batch Processing","text":"<pre><code># Process multiple files efficiently\nnetintel-ocr --batch \\\n             --max-parallel 4 \\\n             --cache-models \\\n             *.pdf\n</code></pre>"},{"location":"customization/#gpu-acceleration","title":"GPU Acceleration","text":"<pre><code># Enable GPU\nexport CUDA_VISIBLE_DEVICES=0\nnetintel-ocr --gpu document.pdf\n\n# Multi-GPU\nexport CUDA_VISIBLE_DEVICES=0,1\nnetintel-ocr --gpu --parallel-pages 2 document.pdf\n</code></pre>"},{"location":"customization/#caching","title":"Caching","text":"<pre><code># Enable result caching\nnetintel-ocr --cache-dir /tmp/netintel-cache document.pdf\n\n# Clear cache\nnetintel-ocr --clear-cache\n</code></pre>"},{"location":"customization/#validation-rules","title":"Validation Rules","text":""},{"location":"customization/#custom-validation","title":"Custom Validation","text":"<pre><code>validation_rules:\n  network:\n    - rule: \"All components must have unique IDs\"\n    - rule: \"Connections must reference existing components\"\n    - rule: \"Security zones must be properly labeled\"\n\n  mermaid:\n    - rule: \"No orphaned nodes\"\n    - rule: \"Valid Mermaid syntax\"\n    - rule: \"Consistent node naming\"\n</code></pre>"},{"location":"customization/#apply-validation","title":"Apply Validation","text":"<pre><code>netintel-ocr --validation-rules custom-rules.yaml document.pdf\n</code></pre>"},{"location":"customization/#additional-resources","title":"Additional Resources","text":"<ul> <li>Full Documentation: https://visionml.net/docs</li> <li>PyPI Package: https://pypi.org/project/netintel-ocr/</li> <li>Example Prompts: https://github.com/VisionMLNet/NetIntelOCR/tree/main/prompts</li> </ul>"},{"location":"customization/#next-steps","title":"Next Steps","text":"<ul> <li>Vector Search Guide - Configure semantic search</li> <li>Deployment Guide - Production setup</li> <li>Quick Start Guide - Basic usage</li> </ul>"},{"location":"deployment/","title":"Deployment Guide","text":"<p>Platform Requirements</p> <p>Linux Only - NetIntel-OCR is tested and supported only on Linux systems.</p> <ul> <li>Supported OS: Ubuntu 20.04/22.04, RHEL 8/9, Debian 11/12</li> <li>Python: 3.11.x or 3.12.x ONLY</li> <li>Architecture: x86_64 (ARM support planned)</li> </ul> <p>Windows and macOS deployments are not currently supported.</p>"},{"location":"deployment/#initialization","title":"Initialization","text":"<p>NetIntel-OCR provides automated deployment configuration:</p> <pre><code># Generate deployment configurations\nnetintel-ocr --init\n\n# Creates:\n# - docker-compose.yml\n# - kubernetes manifests\n# - .env configuration\n</code></pre>"},{"location":"deployment/#docker-deployment","title":"Docker Deployment","text":""},{"location":"deployment/#single-container","title":"Single Container","text":"<pre><code># Use Python 3.11 or 3.12 ONLY\nFROM python:3.11-slim-bullseye\n# OR: FROM python:3.12-slim-bookworm\n\nWORKDIR /app\n\n# Install system dependencies (Linux only)\nRUN apt-get update &amp;&amp; apt-get install -y \\\n    gcc \\\n    g++ \\\n    python3-dev \\\n    &amp;&amp; rm -rf /var/lib/apt/lists/*\n\n# Install NetIntel-OCR from PyPI\n# https://pypi.org/project/netintel-ocr/\nRUN pip install netintel-ocr\n\n# Configure Ollama endpoint\nENV OLLAMA_HOST=http://ollama:11434\n\nCMD [\"netintel-ocr\", \"--api\"]\n</code></pre> <p>Base Image Selection</p> <ul> <li>Use <code>python:3.11-slim-bullseye</code> for Debian 11 base</li> <li>Use <code>python:3.12-slim-bookworm</code> for Debian 12 base</li> <li>Alpine Linux images are not supported due to dependency issues</li> </ul>"},{"location":"deployment/#docker-compose","title":"Docker Compose","text":"<p>Generated <code>docker-compose.yml</code>:</p> <pre><code>version: '3.8'\n\nservices:\n  netintel-ocr:\n    image: netintel-ocr:latest\n    container_name: netintel-ocr\n    environment:\n      - OLLAMA_HOST=http://ollama:11434\n      - API_PORT=8000\n    ports:\n      - \"8000:8000\"\n    volumes:\n      - ./input:/app/input\n      - ./output:/app/output\n      - ./config:/app/config\n    depends_on:\n      - ollama\n    restart: unless-stopped\n\n  ollama:\n    image: ollama/ollama:latest\n    container_name: ollama\n    ports:\n      - \"11434:11434\"\n    volumes:\n      - ollama_data:/root/.ollama\n    deploy:\n      resources:\n        reservations:\n          devices:\n            - driver: nvidia\n              count: 1\n              capabilities: [gpu]\n\n  milvus:\n    image: milvusdb/milvus:latest\n    container_name: milvus\n    ports:\n      - \"19530:19530\"\n      - \"9091:9091\"\n    volumes:\n      - milvus_data:/var/lib/milvus\n    environment:\n      - ETCD_ENDPOINTS=etcd:2379\n      - MINIO_ADDRESS=minio:9000\n\nvolumes:\n  ollama_data:\n  milvus_data:\n</code></pre>"},{"location":"deployment/#build-and-run","title":"Build and Run","text":"<pre><code># Verify Linux system\nuname -s  # Should output \"Linux\"\n\n# Verify Python version in container\ndocker run --rm python:3.11-slim-bullseye python --version\n# Output: Python 3.11.x\n\n# Build image\ndocker build -t netintel-ocr:latest .\n\n# Start services\ndocker-compose up -d\n\n# Check logs\ndocker-compose logs -f netintel-ocr\n</code></pre> <p>Platform Compatibility</p> <p>Docker images built on Linux may not work correctly on Windows/Mac Docker Desktop due to architecture differences.</p>"},{"location":"deployment/#kubernetes-deployment","title":"Kubernetes Deployment","text":""},{"location":"deployment/#configmap","title":"ConfigMap","text":"<pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: netintel-ocr-config\n  namespace: default\ndata:\n  OLLAMA_HOST: \"http://ollama-service:11434\"\n  API_PORT: \"8000\"\n  LOG_LEVEL: \"INFO\"\n</code></pre>"},{"location":"deployment/#deployment","title":"Deployment","text":"<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: netintel-ocr\n  namespace: default\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: netintel-ocr\n  template:\n    metadata:\n      labels:\n        app: netintel-ocr\n    spec:\n      containers:\n      - name: netintel-ocr\n        image: netintel-ocr:latest\n        ports:\n        - containerPort: 8000\n        envFrom:\n        - configMapRef:\n            name: netintel-ocr-config\n        volumeMounts:\n        - name: input\n          mountPath: /app/input\n        - name: output\n          mountPath: /app/output\n        resources:\n          requests:\n            memory: \"4Gi\"\n            cpu: \"2\"\n          limits:\n            memory: \"8Gi\"\n            cpu: \"4\"\n      volumes:\n      - name: input\n        persistentVolumeClaim:\n          claimName: netintel-input-pvc\n      - name: output\n        persistentVolumeClaim:\n          claimName: netintel-output-pvc\n</code></pre>"},{"location":"deployment/#service","title":"Service","text":"<pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: netintel-ocr-service\n  namespace: default\nspec:\n  selector:\n    app: netintel-ocr\n  ports:\n  - protocol: TCP\n    port: 8000\n    targetPort: 8000\n  type: LoadBalancer\n</code></pre>"},{"location":"deployment/#horizontal-pod-autoscaler","title":"Horizontal Pod Autoscaler","text":"<pre><code>apiVersion: autoscaling/v2\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: netintel-ocr-hpa\nspec:\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: netintel-ocr\n  minReplicas: 2\n  maxReplicas: 10\n  metrics:\n  - type: Resource\n    resource:\n      name: cpu\n      target:\n        type: Utilization\n        averageUtilization: 70\n  - type: Resource\n    resource:\n      name: memory\n      target:\n        type: Utilization\n        averageUtilization: 80\n</code></pre>"},{"location":"deployment/#deploy-to-kubernetes","title":"Deploy to Kubernetes","text":"<pre><code># Apply configurations\nkubectl apply -f k8s/\n\n# Check deployment\nkubectl get pods -l app=netintel-ocr\n\n# View logs\nkubectl logs -f deployment/netintel-ocr\n\n# Scale deployment\nkubectl scale deployment netintel-ocr --replicas=5\n</code></pre>"},{"location":"deployment/#production-configuration","title":"Production Configuration","text":""},{"location":"deployment/#system-requirements","title":"System Requirements","text":"<p>Production Environment</p> <ul> <li>OS: Linux x86_64 (Ubuntu 20.04+ or RHEL 8+ recommended)</li> <li>Python: 3.11.x or 3.12.x (verified installation)</li> <li>Memory: 16GB minimum, 32GB recommended</li> <li>CPU: 8+ cores recommended</li> <li>Storage: SSD with 100GB+ available</li> </ul>"},{"location":"deployment/#environment-variables","title":"Environment Variables","text":"<pre><code># Python version check (REQUIRED)\nPYTHON_VERSION=$(python3 --version | cut -d' ' -f2)\n# Must be 3.11.x or 3.12.x\n\n# Required\nOLLAMA_HOST=http://ollama.internal:11434\nMILVUS_HOST=milvus.internal:19530\n\n# Optional\nAPI_PORT=8000\nLOG_LEVEL=INFO\nMAX_WORKERS=4\nTIMEOUT_SECONDS=300\nCACHE_DIR=/app/cache\n</code></pre>"},{"location":"deployment/#health-checks","title":"Health Checks","text":"<pre><code>livenessProbe:\n  httpGet:\n    path: /health\n    port: 8000\n  initialDelaySeconds: 30\n  periodSeconds: 10\n\nreadinessProbe:\n  httpGet:\n    path: /ready\n    port: 8000\n  initialDelaySeconds: 5\n  periodSeconds: 5\n</code></pre>"},{"location":"deployment/#monitoring","title":"Monitoring","text":"<pre><code># Prometheus metrics\n- port: 9090\n  name: metrics\n  annotations:\n    prometheus.io/scrape: \"true\"\n    prometheus.io/path: \"/metrics\"\n</code></pre>"},{"location":"deployment/#api-mode","title":"API Mode","text":""},{"location":"deployment/#start-api-server","title":"Start API Server","text":"<pre><code># Local\nnetintel-ocr --api --port 8000\n\n# Docker\ndocker run -p 8000:8000 netintel-ocr:latest --api\n</code></pre>"},{"location":"deployment/#api-endpoints","title":"API Endpoints","text":"<pre><code># Process document\ncurl -X POST http://localhost:8000/process \\\n  -F \"file=@document.pdf\" \\\n  -F \"model=qwen2.5vl:7b\"\n\n# Check status\ncurl http://localhost:8000/status/job-id\n\n# Get results\ncurl http://localhost:8000/results/job-id\n</code></pre>"},{"location":"deployment/#storage-configuration","title":"Storage Configuration","text":""},{"location":"deployment/#persistent-volumes","title":"Persistent Volumes","text":"<pre><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: netintel-output-pvc\nspec:\n  accessModes:\n    - ReadWriteMany\n  storageClassName: nfs-storage\n  resources:\n    requests:\n      storage: 100Gi\n</code></pre>"},{"location":"deployment/#s3-integration","title":"S3 Integration","text":"<pre><code># Configure S3 backend\nexport AWS_ACCESS_KEY_ID=your-key\nexport AWS_SECRET_ACCESS_KEY=your-secret\nexport S3_BUCKET=netintel-output\nexport S3_REGION=us-east-1\n</code></pre>"},{"location":"deployment/#next-steps","title":"Next Steps","text":"<ul> <li>Customization Guide - Configure prompts and models</li> <li>Vector Search Guide - Set up Milvus integration</li> <li>Quick Start Guide - Basic usage examples</li> </ul>"},{"location":"mcp/","title":"MCP Server Guide","text":""},{"location":"mcp/#model-context-protocol-integration","title":"Model Context Protocol Integration","text":"<p>NetIntel-OCR supports the Model Context Protocol (MCP) for seamless integration with AI assistants and LLM applications.</p>"},{"location":"mcp/#starting-mcp-server","title":"Starting MCP Server","text":""},{"location":"mcp/#basic-setup","title":"Basic Setup","text":"<pre><code># Start MCP server\nnetintel-ocr --mcp\n\n# With custom configuration\nnetintel-ocr --mcp --mcp-port 3000\n</code></pre>"},{"location":"mcp/#configuration-file","title":"Configuration File","text":"<pre><code>{\n  \"mcp\": {\n    \"enabled\": true,\n    \"port\": 3000,\n    \"tools\": [\n      \"process_document\",\n      \"search_diagrams\",\n      \"extract_components\"\n    ],\n    \"auth\": {\n      \"type\": \"bearer\",\n      \"token\": \"mcp-secret-token\"\n    }\n  }\n}\n</code></pre>"},{"location":"mcp/#available-tools","title":"Available Tools","text":""},{"location":"mcp/#process_document","title":"process_document","text":"<p>Process PDF documents and extract network/flow diagrams.</p> <pre><code>{\n  \"tool\": \"process_document\",\n  \"parameters\": {\n    \"file_path\": \"/path/to/document.pdf\",\n    \"start_page\": 1,\n    \"end_page\": 10,\n    \"model\": \"qwen2.5vl:7b\"\n  }\n}\n</code></pre>"},{"location":"mcp/#search_diagrams","title":"search_diagrams","text":"<p>Search for specific network components or patterns.</p> <pre><code>{\n  \"tool\": \"search_diagrams\", \n  \"parameters\": {\n    \"query\": \"firewall DMZ configuration\",\n    \"limit\": 5,\n    \"filters\": {\n      \"type\": \"network_diagram\"\n    }\n  }\n}\n</code></pre>"},{"location":"mcp/#extract_components","title":"extract_components","text":"<p>Extract specific components from diagrams.</p> <pre><code>{\n  \"tool\": \"extract_components\",\n  \"parameters\": {\n    \"image_path\": \"/path/to/diagram.png\",\n    \"component_types\": [\"firewall\", \"router\", \"switch\"]\n  }\n}\n</code></pre>"},{"location":"mcp/#claude-desktop-integration","title":"Claude Desktop Integration","text":""},{"location":"mcp/#configuration","title":"Configuration","text":"<p>Add to Claude Desktop config (<code>~/.config/claude/config.json</code>):</p> <pre><code>{\n  \"mcpServers\": {\n    \"netintel-ocr\": {\n      \"command\": \"netintel-ocr\",\n      \"args\": [\"--mcp\"],\n      \"env\": {\n        \"OLLAMA_HOST\": \"http://localhost:11434\"\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"mcp/#usage-in-claude","title":"Usage in Claude","text":"<pre><code>User: Process the network architecture document and find all firewall configurations.\n\nClaude: I'll process the document using NetIntel-OCR to extract network diagrams and identify firewall configurations.\n\n[Uses process_document tool]\n[Uses search_diagrams tool with \"firewall\" query]\n\nI found 3 network diagrams containing firewall configurations:\n1. Page 5: DMZ architecture with dual firewalls\n2. Page 12: Internal segmentation firewall\n3. Page 18: Cloud gateway firewall setup\n</code></pre>"},{"location":"mcp/#vs-code-integration","title":"VS Code Integration","text":""},{"location":"mcp/#extension-setup","title":"Extension Setup","text":"<pre><code>{\n  \"mcp.servers\": [\n    {\n      \"name\": \"netintel-ocr\",\n      \"command\": \"netintel-ocr --mcp\",\n      \"env\": {\n        \"OLLAMA_HOST\": \"http://localhost:11434\"\n      }\n    }\n  ]\n}\n</code></pre>"},{"location":"mcp/#commands","title":"Commands","text":"<ul> <li><code>MCP: Process Current File</code> - Process open PDF</li> <li><code>MCP: Search Diagrams</code> - Search across processed documents</li> <li><code>MCP: Extract Components</code> - Extract from selected image</li> </ul>"},{"location":"mcp/#programmatic-usage","title":"Programmatic Usage","text":""},{"location":"mcp/#python-mcp-client","title":"Python MCP Client","text":"<pre><code>from mcp_client import MCPClient\n\n# Connect to MCP server\nclient = MCPClient(\"localhost:3000\")\n\n# Use tools\nresult = await client.call_tool(\n    \"process_document\",\n    {\n        \"file_path\": \"network.pdf\",\n        \"model\": \"qwen2.5vl:7b\"\n    }\n)\n\n# Get network diagrams\ndiagrams = result['diagrams']\nfor diagram in diagrams:\n    print(f\"Page {diagram['page']}: {diagram['type']}\")\n    print(diagram['mermaid'])\n</code></pre>"},{"location":"mcp/#nodejs-mcp-client","title":"Node.js MCP Client","text":"<pre><code>const { MCPClient } = require('@modelcontextprotocol/client');\n\nconst client = new MCPClient({\n  url: 'http://localhost:3000'\n});\n\n// Process document\nconst result = await client.callTool('process_document', {\n  file_path: 'network.pdf'\n});\n\nconsole.log(`Found ${result.diagrams.length} diagrams`);\n</code></pre>"},{"location":"mcp/#tool-responses","title":"Tool Responses","text":""},{"location":"mcp/#structured-output","title":"Structured Output","text":"<p>All tools return structured JSON responses:</p> <pre><code>{\n  \"success\": true,\n  \"data\": {\n    \"diagrams\": [\n      {\n        \"page\": 5,\n        \"type\": \"network_topology\",\n        \"confidence\": 0.95,\n        \"mermaid\": \"graph TB...\",\n        \"components\": [...],\n        \"context\": {\n          \"summary\": \"DMZ network architecture\",\n          \"security_zones\": [\"DMZ\", \"Internal\", \"External\"]\n        }\n      }\n    ],\n    \"processing_time\": 12.5\n  }\n}\n</code></pre>"},{"location":"mcp/#error-handling","title":"Error Handling","text":"<pre><code>{\n  \"success\": false,\n  \"error\": {\n    \"code\": \"MODEL_NOT_FOUND\",\n    \"message\": \"Model 'unknown' not available\",\n    \"suggestions\": [\"qwen2.5vl:7b\", \"llava:13b\"]\n  }\n}\n</code></pre>"},{"location":"mcp/#advanced-features","title":"Advanced Features","text":""},{"location":"mcp/#streaming-responses","title":"Streaming Responses","text":"<pre><code># Enable streaming for large documents\nasync for chunk in client.stream_tool(\n    \"process_document\",\n    {\"file_path\": \"large.pdf\", \"stream\": True}\n):\n    print(f\"Processing page {chunk['page']}\")\n</code></pre>"},{"location":"mcp/#context-management","title":"Context Management","text":"<pre><code># Maintain context across calls\ncontext = client.create_context()\n\n# First call\nawait context.call_tool(\"process_document\", {...})\n\n# Subsequent call uses previous context\nawait context.call_tool(\"search_diagrams\", {\n    \"query\": \"related to previous document\"\n})\n</code></pre>"},{"location":"mcp/#custom-tools","title":"Custom Tools","text":"<p>Register custom tools for specific workflows:</p> <pre><code>@mcp_server.register_tool\ndef analyze_security(params):\n    \"\"\"Analyze security aspects of network diagrams.\"\"\"\n    return {\n        \"vulnerabilities\": [...],\n        \"recommendations\": [...],\n        \"risk_score\": 7.5\n    }\n</code></pre>"},{"location":"mcp/#all-in-one-mode","title":"All-in-One Mode","text":"<p>Start with all services:</p> <pre><code># MCP + API + Vector Store\nnetintel-ocr --all-in-one\n\n# Services available:\n# - MCP Server: port 3000\n# - REST API: port 8000\n# - Milvus: port 19530\n</code></pre>"},{"location":"mcp/#security","title":"Security","text":""},{"location":"mcp/#authentication","title":"Authentication","text":"<pre><code># Token authentication\nnetintel-ocr --mcp --mcp-token YOUR_SECRET_TOKEN\n\n# mTLS authentication\nnetintel-ocr --mcp \\\n  --mcp-cert server.crt \\\n  --mcp-key server.key \\\n  --mcp-ca ca.crt\n</code></pre>"},{"location":"mcp/#access-control","title":"Access Control","text":"<pre><code>{\n  \"mcp\": {\n    \"acl\": {\n      \"process_document\": [\"admin\", \"user\"],\n      \"search_diagrams\": [\"admin\", \"user\", \"readonly\"],\n      \"extract_components\": [\"admin\"]\n    }\n  }\n}\n</code></pre>"},{"location":"mcp/#monitoring","title":"Monitoring","text":""},{"location":"mcp/#metrics","title":"Metrics","text":"<pre><code>GET http://localhost:3000/metrics\n\nmcp_requests_total{tool=\"process_document\"} 156\nmcp_request_duration_seconds{tool=\"process_document\",quantile=\"0.99\"} 12.5\nmcp_active_connections 3\n</code></pre>"},{"location":"mcp/#logging","title":"Logging","text":"<pre><code># Enable debug logging\nnetintel-ocr --mcp --mcp-log-level DEBUG\n\n# Log format\n2024-01-15 10:30:45 [MCP] Tool called: process_document\n2024-01-15 10:30:57 [MCP] Response sent: success=true, time=12.5s\n</code></pre>"},{"location":"mcp/#next-steps","title":"Next Steps","text":"<ul> <li>Multi-Model Guide - Optimize model selection</li> <li>API Integration - REST API reference</li> <li>Batch Processing - Process multiple documents</li> </ul>"},{"location":"multi-model/","title":"Multi-Model Selection Guide","text":""},{"location":"multi-model/#overview","title":"Overview","text":"<p>NetIntel-OCR supports multiple vision-language models optimized for different tasks. Selecting the right model improves accuracy and performance.</p>"},{"location":"multi-model/#model-categories","title":"Model Categories","text":""},{"location":"multi-model/#ocr-optimized-models","title":"OCR-Optimized Models","text":"<p>Best for text extraction from documents.</p> Model Speed Accuracy Memory Use Case <code>Nanonets-OCR-s:latest</code> \u26a1\u26a1\u26a1 High 4GB Default OCR <code>moondream:latest</code> \u26a1\u26a1 Medium 3GB Fast processing <code>NetIntelOCR-7B-0925</code> \u26a1\u26a1 Very High 8GB Default (v0.1.16)"},{"location":"multi-model/#vision-language-models","title":"Vision-Language Models","text":"<p>Best for diagram understanding and component extraction.</p> Model Speed Accuracy Memory Use Case <code>qwen2.5vl:7b</code> \u26a1\u26a1 Very High 8GB Recommended <code>llava:13b</code> \u26a1 Highest 16GB Complex diagrams <code>cogvlm:latest</code> Slow Highest 32GB Critical accuracy <code>minicpm-v:latest</code> \u26a1\u26a1\u26a1 Medium 4GB Quick preview"},{"location":"multi-model/#lightweight-models","title":"Lightweight Models","text":"<p>Best for quick detection and simple diagrams.</p> Model Speed Accuracy Memory Use Case <code>bakllava:latest</code> \u26a1\u26a1\u26a1 Medium 4GB Fast detection <code>llava-phi3:latest</code> \u26a1\u26a1\u26a1 Medium 3GB Edge deployment <code>llama3.2-vision:11b</code> \u26a1 High 12GB Balanced"},{"location":"multi-model/#task-specific-recommendations","title":"Task-Specific Recommendations","text":""},{"location":"multi-model/#text-extraction","title":"Text Extraction","text":"<pre><code># Fast text extraction\nnetintel-ocr --model moondream:latest document.pdf\n\n# High accuracy OCR\nnetintel-ocr --model Nanonets-OCR-s:latest document.pdf\n\n# Default balanced approach\nnetintel-ocr --model NetIntelOCR-7B-0925 document.pdf\n</code></pre>"},{"location":"multi-model/#network-diagrams","title":"Network Diagrams","text":"<pre><code># Simple network topology\nnetintel-ocr --network-model minicpm-v:latest network.pdf\n\n# Complex architecture\nnetintel-ocr --network-model llava:13b architecture.pdf\n\n# Recommended for most cases\nnetintel-ocr --network-model qwen2.5vl:7b design.pdf\n</code></pre>"},{"location":"multi-model/#flow-diagrams","title":"Flow Diagrams","text":"<pre><code># Business process flows\nnetintel-ocr --flow-model qwen2.5vl:7b process.pdf\n\n# Complex decision trees\nnetintel-ocr --flow-model llava:13b workflow.pdf\n\n# Quick extraction\nnetintel-ocr --flow-model bakllava:latest simple-flow.pdf\n</code></pre>"},{"location":"multi-model/#model-selection-strategy","title":"Model Selection Strategy","text":""},{"location":"multi-model/#by-document-type","title":"By Document Type","text":""},{"location":"multi-model/#technical-specifications","title":"Technical Specifications","text":"<pre><code>netintel-ocr \\\n  --model Nanonets-OCR-s:latest \\\n  --network-model cogvlm:latest \\\n  --flow-model llava:13b \\\n  technical-spec.pdf\n</code></pre>"},{"location":"multi-model/#marketing-materials","title":"Marketing Materials","text":"<pre><code>netintel-ocr \\\n  --model moondream:latest \\\n  --network-model minicpm-v:latest \\\n  --flow-model bakllava:latest \\\n  brochure.pdf\n</code></pre>"},{"location":"multi-model/#security-documentation","title":"Security Documentation","text":"<pre><code>netintel-ocr \\\n  --model NetIntelOCR-7B-0925 \\\n  --network-model qwen2.5vl:7b \\\n  --security-focus \\\n  security-guide.pdf\n</code></pre>"},{"location":"multi-model/#by-resource-constraints","title":"By Resource Constraints","text":""},{"location":"multi-model/#limited-memory-4gb","title":"Limited Memory (4GB)","text":"<pre><code>netintel-ocr \\\n  --model moondream:latest \\\n  --network-model minicpm-v:latest \\\n  --low-memory \\\n  document.pdf\n</code></pre>"},{"location":"multi-model/#gpu-available","title":"GPU Available","text":"<pre><code>netintel-ocr \\\n  --model NetIntelOCR-7B-0925 \\\n  --network-model llava:13b \\\n  --gpu \\\n  document.pdf\n</code></pre>"},{"location":"multi-model/#cpu-only","title":"CPU Only","text":"<pre><code>netintel-ocr \\\n  --model Nanonets-OCR-s:latest \\\n  --network-model bakllava:latest \\\n  --cpu-optimized \\\n  document.pdf\n</code></pre>"},{"location":"multi-model/#model-configuration","title":"Model Configuration","text":""},{"location":"multi-model/#default-models","title":"Default Models","text":"<p>Set default models in configuration:</p> <pre><code># config.yaml\nmodels:\n  text_extraction: NetIntelOCR-7B-0925\n  network_detection: qwen2.5vl:7b\n  flow_detection: qwen2.5vl:7b\n  component_extraction: qwen2.5vl:7b\n\nfallbacks:\n  text_extraction: Nanonets-OCR-s:latest\n  network_detection: minicpm-v:latest\n</code></pre>"},{"location":"multi-model/#model-specific-parameters","title":"Model-Specific Parameters","text":"<pre><code>model_configs:\n  qwen2.5vl:\n    temperature: 0.3\n    max_tokens: 4096\n    top_p: 0.9\n\n  llava:\n    temperature: 0.5\n    max_tokens: 8192\n    num_predict: 2048\n\n  NetIntelOCR-7B-0925:\n    temperature: 0.2\n    repeat_penalty: 1.1\n</code></pre>"},{"location":"multi-model/#performance-optimization","title":"Performance Optimization","text":""},{"location":"multi-model/#batch-processing","title":"Batch Processing","text":"<pre><code># Use fast models for batch\nnetintel-ocr --batch \\\n  --model moondream:latest \\\n  --network-model minicpm-v:latest \\\n  *.pdf\n</code></pre>"},{"location":"multi-model/#multi-pass-strategy","title":"Multi-Pass Strategy","text":"<pre><code># First pass: Quick detection\nnetintel-ocr --detect-only \\\n  --network-model bakllava:latest \\\n  document.pdf\n\n# Second pass: Detailed extraction on detected pages\nnetintel-ocr --pages 5,12,18 \\\n  --network-model llava:13b \\\n  document.pdf\n</code></pre>"},{"location":"multi-model/#model-caching","title":"Model Caching","text":"<pre><code># Preload models\nnetintel-ocr --preload-models \\\n  \"qwen2.5vl:7b,Nanonets-OCR-s:latest\"\n\n# Keep models in memory\nnetintel-ocr --keep-models-loaded \\\n  --model-cache-ttl 3600 \\\n  document.pdf\n</code></pre>"},{"location":"multi-model/#model-benchmarks","title":"Model Benchmarks","text":""},{"location":"multi-model/#processing-speed-pagesminute","title":"Processing Speed (pages/minute)","text":"Task Nanonets qwen2.5vl llava minicpm-v Text Only 12 8 4 15 Simple Diagram 8 6 3 10 Complex Diagram 4 4 2 6"},{"location":"multi-model/#accuracy-scores-f1","title":"Accuracy Scores (F1)","text":"Task Nanonets qwen2.5vl llava minicpm-v Text OCR 0.95 0.92 0.94 0.85 Component Detection 0.82 0.91 0.94 0.78 Connection Tracing 0.75 0.88 0.92 0.72"},{"location":"multi-model/#custom-model-integration","title":"Custom Model Integration","text":""},{"location":"multi-model/#add-custom-model","title":"Add Custom Model","text":"<pre><code># Download and configure\nollama pull your-custom-model:latest\n\n# Register with NetIntel-OCR\nnetintel-ocr --register-model \\\n  --name custom-model \\\n  --type vision-language \\\n  --capabilities \"network,flow,text\"\n</code></pre>"},{"location":"multi-model/#model-evaluation","title":"Model Evaluation","text":"<pre><code># Test model performance\nnetintel-ocr --evaluate-model custom-model:latest \\\n  --test-set /path/to/test/documents \\\n  --metrics \"accuracy,speed,memory\"\n</code></pre>"},{"location":"multi-model/#troubleshooting-models","title":"Troubleshooting Models","text":""},{"location":"multi-model/#model-not-found","title":"Model Not Found","text":"<pre><code># List available models\nollama list\n\n# Pull missing model\nollama pull qwen2.5vl:7b\n</code></pre>"},{"location":"multi-model/#out-of-memory","title":"Out of Memory","text":"<pre><code># Use smaller model\nnetintel-ocr --network-model minicpm-v:latest document.pdf\n\n# Reduce context size\nnetintel-ocr --max-context 2048 document.pdf\n</code></pre>"},{"location":"multi-model/#slow-processing","title":"Slow Processing","text":"<pre><code># Use faster model\nnetintel-ocr --network-model bakllava:latest document.pdf\n\n# Enable GPU\nnetintel-ocr --gpu document.pdf\n\n# Reduce quality for speed\nnetintel-ocr --fast-mode document.pdf\n</code></pre>"},{"location":"multi-model/#next-steps","title":"Next Steps","text":"<ul> <li>Customization Guide - Fine-tune model parameters</li> <li>Performance Guide - Optimize for large batches</li> <li>Troubleshooting - Common issues and solutions</li> </ul>"},{"location":"quickstart/","title":"Quick Start Guide","text":""},{"location":"quickstart/#installation","title":"Installation","text":""},{"location":"quickstart/#system-requirements","title":"System Requirements","text":"<p>Operating System Support</p> <p>Linux Only - NetIntel-OCR is currently tested and supported only on Linux distributions.</p> <p>Windows and macOS support is planned for future releases but not currently available.</p> <p>Python Version</p> <p>Python 3.11 or 3.12 Required - NetIntel-OCR is tested and supported only on Python 3.11 and 3.12.</p> <p>Other Python versions may work but are not officially supported.</p>"},{"location":"quickstart/#verified-configurations","title":"Verified Configurations","text":"<ul> <li>OS: Ubuntu 20.04/22.04, RHEL 8/9, Debian 11/12</li> <li>Python: 3.11.x or 3.12.x</li> <li>RAM: 8GB minimum (16GB recommended)</li> <li>Storage: 10GB for models + processing space</li> <li>Ollama: Version 0.1.0 or higher</li> </ul>"},{"location":"quickstart/#python-setup","title":"Python Setup","text":"<pre><code># Check Python version (must be 3.11 or 3.12)\npython3 --version\n\n# Install Python 3.11 on Ubuntu/Debian\nsudo apt update\nsudo apt install python3.11 python3.11-venv python3.11-dev\n\n# Or install Python 3.12\nsudo apt install python3.12 python3.12-venv python3.12-dev\n\n# Create virtual environment with Python 3.11\npython3.11 -m venv venv\nsource venv/bin/activate\n\n# Or with Python 3.12\npython3.12 -m venv venv\nsource venv/bin/activate\n</code></pre>"},{"location":"quickstart/#install-netintel-ocr","title":"Install NetIntel-OCR","text":"<pre><code># Ensure you're using Python 3.11 or 3.12\npython --version  # Should show 3.11.x or 3.12.x\n\n# Install latest version from PyPI\npip install netintel-ocr\n\n# Or install specific version\npip install netintel-ocr==0.1.16.15\n\n# Verify installation\nnetintel-ocr --version\n</code></pre> <p>Package Information</p> <p>NetIntel-OCR is available on PyPI: https://pypi.org/project/netintel-ocr/</p> <p>Check for latest versions and release notes on the PyPI page.</p>"},{"location":"quickstart/#install-ollama-models","title":"Install Ollama Models","text":"<p>NetIntel-OCR requires vision-capable models for diagram processing:</p> <pre><code># Pull recommended models\nollama pull qwen2.5vl:7b        # For network diagrams\nollama pull Nanonets-OCR-s:latest  # For OCR processing\n\n# Optional: Pull alternative models\nollama pull llava:13b            # Alternative vision model\nollama pull minicpm-v:latest     # Lightweight option\n</code></pre>"},{"location":"quickstart/#configuration","title":"Configuration","text":""},{"location":"quickstart/#setting-ollama-host","title":"Setting Ollama Host","text":"<p>For remote Ollama servers:</p> <pre><code># Set environment variable\nexport OLLAMA_HOST=\"http://192.168.1.100:11434\"\n\n# Or use config file\nnetintel-ocr --init\n</code></pre>"},{"location":"quickstart/#model-selection","title":"Model Selection","text":"<p>Specify models for different processing tasks:</p> Parameter Purpose Recommended Model <code>--model</code> OCR and text extraction Nanonets-OCR-s:latest <code>--network-model</code> Network diagram analysis qwen2.5vl:7b <code>--flow-model</code> Flow chart processing qwen2.5vl:7b"},{"location":"quickstart/#basic-usage","title":"Basic Usage","text":""},{"location":"quickstart/#process-a-pdf-document","title":"Process a PDF Document","text":"<pre><code># Process entire document\nnetintel-ocr document.pdf\n\n# Process specific pages\nnetintel-ocr --start 5 --end 10 document.pdf\n\n# Enable debug output\nnetintel-ocr --debug document.pdf\n</code></pre>"},{"location":"quickstart/#example-network-architecture-document","title":"Example: Network Architecture Document","text":"<pre><code>netintel-ocr \\\n  --model Nanonets-OCR-s:latest \\\n  --network-model qwen2.5vl:7b \\\n  --debug \\\n  cisco-sdwan-design-guide.pdf \\\n  --start 5 --end 10\n</code></pre> <p>Output structure: <pre><code>output/\n\u251c\u2500\u2500 cisco-sdwan-design-guide/\n\u2502   \u251c\u2500\u2500 page_005.md          # Extracted text\n\u2502   \u251c\u2500\u2500 page_006_network.md  # Network diagram with Mermaid\n\u2502   \u251c\u2500\u2500 page_007.md          # Regular page\n\u2502   \u2514\u2500\u2500 summary.json         # Processing summary\n</code></pre></p>"},{"location":"quickstart/#output-formats","title":"Output Formats","text":""},{"location":"quickstart/#markdown-files","title":"Markdown Files","text":"<p>Each page generates a markdown file containing: - Extracted text content - Mermaid diagrams for network/flow charts - Context analysis and interpretations - Component and connection listings</p>"},{"location":"quickstart/#mermaid-diagrams","title":"Mermaid Diagrams","text":"<p>Network diagrams are converted to Mermaid syntax:</p> <pre><code>graph TB\n    Router[\"Core Router\"]\n    Switch1[\"Access Switch 1\"]\n    Switch2[\"Access Switch 2\"]\n\n    Router --&gt; Switch1\n    Router --&gt; Switch2</code></pre>"},{"location":"quickstart/#summary-json","title":"Summary JSON","text":"<p>Processing statistics and metadata:</p> <pre><code>{\n  \"total_pages\": 10,\n  \"network_diagrams\": 3,\n  \"flow_diagrams\": 2,\n  \"processing_time\": 45.2,\n  \"models_used\": {\n    \"ocr\": \"Nanonets-OCR-s:latest\",\n    \"network\": \"qwen2.5vl:7b\"\n  }\n}\n</code></pre>"},{"location":"quickstart/#common-use-cases","title":"Common Use Cases","text":""},{"location":"quickstart/#extract-network-topologies","title":"Extract Network Topologies","text":"<pre><code>netintel-ocr \\\n  --network-only \\\n  --network-model qwen2.5vl:7b \\\n  network-design.pdf\n</code></pre>"},{"location":"quickstart/#process-security-documentation","title":"Process Security Documentation","text":"<pre><code>netintel-ocr \\\n  --include-context \\\n  --extract-security \\\n  firewall-config.pdf\n</code></pre>"},{"location":"quickstart/#batch-processing","title":"Batch Processing","text":"<pre><code># Process multiple documents\nfor pdf in *.pdf; do\n  netintel-ocr --output-dir processed/ \"$pdf\"\ndone\n</code></pre>"},{"location":"quickstart/#troubleshooting","title":"Troubleshooting","text":""},{"location":"quickstart/#model-not-found","title":"Model Not Found","text":"<pre><code># Check available models\nollama list\n\n# Pull missing model\nollama pull qwen2.5vl:7b\n</code></pre>"},{"location":"quickstart/#connection-issues","title":"Connection Issues","text":"<pre><code># Test Ollama connection\ncurl http://localhost:11434/api/tags\n\n# Check environment\necho $OLLAMA_HOST\n</code></pre>"},{"location":"quickstart/#performance-optimization","title":"Performance Optimization","text":"<pre><code># Use faster model for testing\nnetintel-ocr --network-model minicpm-v:latest document.pdf\n\n# Limit pages for large documents\nnetintel-ocr --start 1 --end 20 large-document.pdf\n\n# Disable context extraction for speed\nnetintel-ocr --no-context document.pdf\n</code></pre>"},{"location":"quickstart/#next-steps","title":"Next Steps","text":"<ul> <li>Deployment Guide - Set up production environment</li> <li>Customization Guide - Tune prompts and extraction</li> <li>Vector Search Guide - Enable semantic search</li> </ul>"},{"location":"troubleshooting/","title":"Troubleshooting Guide","text":"<p>Platform Requirements</p> <p>Linux Only - NetIntel-OCR runs only on Linux systems.</p> <p>Python 3.11 or 3.12 Only - Other versions are not supported.</p> <p>If you're on Windows/Mac or using a different Python version, these are likely the cause of your issues.</p>"},{"location":"troubleshooting/#common-issues-and-solutions","title":"Common Issues and Solutions","text":""},{"location":"troubleshooting/#platform-issues","title":"Platform Issues","text":""},{"location":"troubleshooting/#wrong-operating-system","title":"Wrong Operating System","text":"<p><pre><code>ERROR: Unsupported platform detected\nERROR: Module compilation failed\nERROR: Binary wheel not available\n</code></pre> Solution: <pre><code># Check your OS (must be Linux)\nuname -s\n# Expected: Linux\n\n# NetIntel-OCR requires Linux. Options:\n# 1. Use WSL2 on Windows\n# 2. Use a Linux VM\n# 3. Use a Linux container\n# 4. Deploy to a Linux server\n</code></pre></p>"},{"location":"troubleshooting/#wrong-python-version","title":"Wrong Python Version","text":"<p><pre><code>ERROR: Python 3.11 or 3.12 required\nERROR: Unsupported Python version\nERROR: Module 'netintel_ocr' has no attribute...\n</code></pre> Solution: <pre><code># Check Python version (MUST be 3.11.x or 3.12.x)\npython --version\n\n# Install Python 3.11 on Ubuntu/Debian\nsudo apt update &amp;&amp; sudo apt install python3.11 python3.11-venv python3.11-dev\n\n# Install Python 3.12 on Ubuntu/Debian  \nsudo apt install python3.12 python3.12-venv python3.12-dev\n\n# Create virtual environment with correct version\npython3.11 -m venv venv\nsource venv/bin/activate\n\n# Verify version in virtual environment\npython --version  # Must show 3.11.x or 3.12.x\n</code></pre></p>"},{"location":"troubleshooting/#missing-dependencies","title":"Missing Dependencies","text":"<p><pre><code>ERROR: No module named 'pymupdf'\nERROR: ImportError: libGL.so.1: cannot open shared object file\n</code></pre> Solution: <pre><code># Install Linux system dependencies first\nsudo apt-get update\nsudo apt-get install -y \\\n    python3.11-dev \\  # or python3.12-dev\n    gcc g++ \\\n    libgl1-mesa-glx \\\n    libglib2.0-0 \\\n    libsm6 libxext6 libxrender-dev \\\n    libgomp1\n\n# Then reinstall with all dependencies\npip install --upgrade netintel-ocr[all]\n\n# Or install missing module\npip install pymupdf pillow opencv-python-headless\n</code></pre></p>"},{"location":"troubleshooting/#c-extension-build-failure","title":"C++ Extension Build Failure","text":"<p><pre><code>ERROR: Failed building wheel for netintel-ocr\nERROR: Microsoft Visual C++ 14.0 is required (Windows error)\n</code></pre> Solution: <pre><code># This is a Windows error - NetIntel-OCR only supports Linux\n# Switch to a Linux environment\n\n# On Linux, install build tools:\nsudo apt-get install build-essential python3.11-dev\n# or\nsudo yum install gcc gcc-c++ python3.11-devel\n</code></pre></p>"},{"location":"troubleshooting/#ollama-connection-issues","title":"Ollama Connection Issues","text":""},{"location":"troubleshooting/#ollama-not-running","title":"Ollama Not Running","text":"<p><pre><code>ERROR: Connection refused to localhost:11434\n</code></pre> Solution: <pre><code># Start Ollama\nollama serve\n\n# Check if running\ncurl http://localhost:11434/api/tags\n\n# Start in background\nnohup ollama serve &gt; ollama.log 2&gt;&amp;1 &amp;\n</code></pre></p>"},{"location":"troubleshooting/#remote-ollama-host","title":"Remote Ollama Host","text":"<p><pre><code>ERROR: Cannot connect to Ollama\n</code></pre> Solution: <pre><code># Set environment variable\nexport OLLAMA_HOST=http://192.168.1.100:11434\n\n# Or use CLI parameter\nnetintel-ocr --ollama-host http://192.168.1.100:11434 document.pdf\n\n# Test connection\ncurl $OLLAMA_HOST/api/tags\n</code></pre></p>"},{"location":"troubleshooting/#model-issues","title":"Model Issues","text":""},{"location":"troubleshooting/#model-not-found","title":"Model Not Found","text":"<p><pre><code>ERROR: Model 'qwen2.5vl:7b' not found\n</code></pre> Solution: <pre><code># List available models\nollama list\n\n# Pull missing model\nollama pull qwen2.5vl:7b\n\n# Use available model\nnetintel-ocr --network-model llava:latest document.pdf\n</code></pre></p>"},{"location":"troubleshooting/#out-of-memory","title":"Out of Memory","text":"<p><pre><code>ERROR: Out of memory (OOM)\n</code></pre> Solution: <pre><code># Use smaller model\nnetintel-ocr --network-model minicpm-v:latest document.pdf\n\n# Limit context size\nnetintel-ocr --max-context 2048 document.pdf\n\n# Process fewer pages\nnetintel-ocr --start 1 --end 10 document.pdf\n\n# Free memory\nollama stop all\n</code></pre></p>"},{"location":"troubleshooting/#processing-errors","title":"Processing Errors","text":""},{"location":"troubleshooting/#pdf-read-error","title":"PDF Read Error","text":"<p><pre><code>ERROR: Cannot read PDF file\n</code></pre> Solution: <pre><code># Check file exists and is valid\nfile document.pdf\npdfinfo document.pdf\n\n# Try repairing PDF\nqpdf --replace-input document.pdf\n\n# Convert with ghostscript\ngs -sDEVICE=pdfwrite -o fixed.pdf document.pdf\n</code></pre></p>"},{"location":"troubleshooting/#timeout-during-processing","title":"Timeout During Processing","text":"<p><pre><code>ERROR: Processing timeout after 300 seconds\n</code></pre> Solution: <pre><code># Increase timeout\nnetintel-ocr --timeout 600 document.pdf\n\n# Use faster model\nnetintel-ocr --network-model bakllava:latest document.pdf\n\n# Process in smaller chunks\nnetintel-ocr --start 1 --end 20 document.pdf\nnetintel-ocr --start 21 --end 40 document.pdf\n</code></pre></p>"},{"location":"troubleshooting/#mermaid-parse-error","title":"Mermaid Parse Error","text":"<p><pre><code>ERROR: Parse error on line 3\n</code></pre> Solution: <pre><code># Enable auto-fix\nnetintel-ocr --fix-mermaid document.pdf\n\n# Use robust validator\nnetintel-ocr --validate-mermaid document.pdf\n\n# Disable Mermaid generation\nnetintel-ocr --no-mermaid document.pdf\n</code></pre></p>"},{"location":"troubleshooting/#vector-store-issues","title":"Vector Store Issues","text":""},{"location":"troubleshooting/#milvus-connection-failed","title":"Milvus Connection Failed","text":"<p><pre><code>ERROR: Cannot connect to Milvus at localhost:19530\n</code></pre> Solution: <pre><code># Start Milvus\ndocker run -d --name milvus \\\n  -p 19530:19530 \\\n  milvusdb/milvus:latest\n\n# Check connection\npython -c \"from pymilvus import connections; connections.connect()\"\n\n# Use different host\nnetintel-ocr --milvus-host milvus.server:19530 document.pdf\n</code></pre></p>"},{"location":"troubleshooting/#collection-already-exists","title":"Collection Already Exists","text":"<p><pre><code>ERROR: Collection 'network_docs' already exists\n</code></pre> Solution: <pre><code># Drop existing collection\nnetintel-ocr --drop-collection network_docs\n\n# Or use different collection\nnetintel-ocr --collection network_docs_v2 document.pdf\n\n# Append to existing\nnetintel-ocr --append-collection network_docs document.pdf\n</code></pre></p>"},{"location":"troubleshooting/#performance-issues","title":"Performance Issues","text":""},{"location":"troubleshooting/#slow-processing","title":"Slow Processing","text":"<p>Symptoms: Processing takes &gt;30s per page</p> <p>Solutions: <pre><code># Use faster models\nnetintel-ocr --fast-mode document.pdf\n\n# Enable GPU\nnetintel-ocr --gpu document.pdf\n\n# Disable context extraction\nnetintel-ocr --no-context document.pdf\n\n# Use cached models\nnetintel-ocr --cache-models document.pdf\n</code></pre></p>"},{"location":"troubleshooting/#high-memory-usage","title":"High Memory Usage","text":"<p>Symptoms: System becomes unresponsive</p> <p>Solutions: <pre><code># Limit memory usage\nnetintel-ocr --max-memory 4GB document.pdf\n\n# Process sequentially\nnetintel-ocr --no-parallel document.pdf\n\n# Clear cache\nnetintel-ocr --clear-cache\nrm -rf ~/.cache/netintel-ocr\n</code></pre></p>"},{"location":"troubleshooting/#output-issues","title":"Output Issues","text":""},{"location":"troubleshooting/#missing-output-files","title":"Missing Output Files","text":"<p><pre><code>No output files generated\n</code></pre> Solution: <pre><code># Check output directory\nls -la output/\n\n# Specify output directory\nnetintel-ocr --output-dir ./my-output document.pdf\n\n# Check permissions\nchmod 755 output/\nchmod 644 document.pdf\n</code></pre></p>"},{"location":"troubleshooting/#corrupted-mermaid-diagrams","title":"Corrupted Mermaid Diagrams","text":"<p>Symptoms: Mermaid diagrams don't render</p> <p>Solution: <pre><code># Validate Mermaid syntax\nnetintel-ocr --validate-only output/page_005_network.md\n\n# Re-process with fixes\nnetintel-ocr --fix-mermaid --pages 5 document.pdf\n\n# Use different Mermaid version\nnetintel-ocr --mermaid-version 10.0.0 document.pdf\n</code></pre></p>"},{"location":"troubleshooting/#environment-verification","title":"Environment Verification","text":""},{"location":"troubleshooting/#check-system-compatibility","title":"Check System Compatibility","text":"<pre><code># Full system check\necho \"OS: $(uname -s)\"  # Must be Linux\necho \"Arch: $(uname -m)\"  # Should be x86_64\necho \"Python: $(python3 --version)\"  # Must be 3.11.x or 3.12.x\necho \"Distro: $(lsb_release -d 2&gt;/dev/null || cat /etc/os-release | grep PRETTY_NAME)\"\n\n# Expected output:\n# OS: Linux\n# Arch: x86_64  \n# Python: Python 3.11.x (or 3.12.x)\n# Distro: Ubuntu 20.04/22.04 or similar\n</code></pre>"},{"location":"troubleshooting/#python-environment-check","title":"Python Environment Check","text":"<pre><code># Verify Python installation\nwhich python3.11 python3.12 2&gt;/dev/null\n\n# Check available Python versions\nls -la /usr/bin/python3*\n\n# Verify pip version matches Python\npip --version  # Should show (python 3.11) or (python 3.12)\n\n# Check virtual environment\npython -c \"import sys; print(f'Python: {sys.version}')\"\npython -c \"import platform; print(f'Platform: {platform.platform()}')\"\n</code></pre>"},{"location":"troubleshooting/#debugging-commands","title":"Debugging Commands","text":""},{"location":"troubleshooting/#enable-debug-mode","title":"Enable Debug Mode","text":"<pre><code># Full debug output (Linux only)\nnetintel-ocr --debug document.pdf\n\n# Debug specific component\nnetintel-ocr --debug-component network_detection document.pdf\n\n# Save debug logs\nnetintel-ocr --debug --log-file debug.log document.pdf\n</code></pre>"},{"location":"troubleshooting/#test-components","title":"Test Components","text":"<pre><code># Test OCR only\nnetintel-ocr --test-ocr document.pdf\n\n# Test diagram detection only\nnetintel-ocr --test-detection document.pdf\n\n# Dry run without processing\nnetintel-ocr --dry-run document.pdf\n</code></pre>"},{"location":"troubleshooting/#health-checks","title":"Health Checks","text":"<pre><code># System health check\nnetintel-ocr --health-check\n\n# Model availability\nnetintel-ocr --check-models\n\n# Storage status\nnetintel-ocr --check-storage\n</code></pre>"},{"location":"troubleshooting/#log-analysis","title":"Log Analysis","text":""},{"location":"troubleshooting/#common-log-patterns","title":"Common Log Patterns","text":"<pre><code># Find errors in logs\ngrep ERROR netintel.log\n\n# Find timeout issues\ngrep -i timeout netintel.log\n\n# Find model issues\ngrep -E \"(model|ollama)\" netintel.log\n\n# Find memory issues\ngrep -E \"(memory|OOM|RAM)\" netintel.log\n</code></pre>"},{"location":"troubleshooting/#log-levels","title":"Log Levels","text":"<pre><code># Verbose logging\nnetintel-ocr --log-level DEBUG document.pdf\n\n# Only errors\nnetintel-ocr --log-level ERROR document.pdf\n\n# Structured JSON logs\nnetintel-ocr --log-format json document.pdf\n</code></pre>"},{"location":"troubleshooting/#recovery-procedures","title":"Recovery Procedures","text":""},{"location":"troubleshooting/#resume-failed-processing","title":"Resume Failed Processing","text":"<pre><code># Save checkpoint\nnetintel-ocr --checkpoint state.json document.pdf\n\n# Resume from checkpoint\nnetintel-ocr --resume state.json document.pdf\n</code></pre>"},{"location":"troubleshooting/#rebuild-vector-index","title":"Rebuild Vector Index","text":"<pre><code># Rebuild corrupted index\nnetintel-ocr --rebuild-index network_docs\n\n# Verify index integrity\nnetintel-ocr --verify-index network_docs\n</code></pre>"},{"location":"troubleshooting/#reset-configuration","title":"Reset Configuration","text":"<pre><code># Reset to defaults\nnetintel-ocr --reset-config\n\n# Regenerate config\nnetintel-ocr --init --force\n</code></pre>"},{"location":"troubleshooting/#platform-specific-issues","title":"Platform-Specific Issues","text":""},{"location":"troubleshooting/#wsl2-windows-subsystem-for-linux","title":"WSL2 (Windows Subsystem for Linux)","text":"<pre><code># If trying to run on Windows via WSL2\n# Ensure WSL2 is properly configured\nwsl --status\nwsl --list --verbose\n\n# Install Ubuntu 22.04 in WSL2\nwsl --install -d Ubuntu-22.04\n\n# Inside WSL2, install Python 3.11\nsudo apt update\nsudo apt install python3.11 python3.11-venv python3.11-dev\n\n# Install NetIntel-OCR in WSL2 environment\npython3.11 -m pip install netintel-ocr\n</code></pre>"},{"location":"troubleshooting/#docker-on-non-linux-systems","title":"Docker on Non-Linux Systems","text":"<pre><code># Running via Docker on Windows/Mac\n# Note: Performance may be degraded\n\n# Use Linux container mode\ndocker run --platform linux/amd64 \\\n  -v $(pwd):/data \\\n  netintel-ocr:latest document.pdf\n\n# Build for Linux platform\ndocker buildx build --platform linux/amd64 -t netintel-ocr:latest .\n</code></pre>"},{"location":"troubleshooting/#common-linux-distribution-issues","title":"Common Linux Distribution Issues","text":""},{"location":"troubleshooting/#ubuntudebian","title":"Ubuntu/Debian","text":"<pre><code># Missing Python 3.11/3.12\nsudo add-apt-repository ppa:deadsnakes/ppa\nsudo apt update\nsudo apt install python3.11-full\n</code></pre>"},{"location":"troubleshooting/#rhelcentosrocky-linux","title":"RHEL/CentOS/Rocky Linux","text":"<pre><code># Enable EPEL and install Python 3.11\nsudo dnf install epel-release\nsudo dnf install python3.11 python3.11-devel\n</code></pre>"},{"location":"troubleshooting/#arch-linux","title":"Arch Linux","text":"<pre><code># Python versions available in standard repos\nsudo pacman -S python  # Usually latest version\n</code></pre>"},{"location":"troubleshooting/#getting-help","title":"Getting Help","text":""},{"location":"troubleshooting/#generate-diagnostic-report","title":"Generate Diagnostic Report","text":"<pre><code># Full system diagnostic (Linux only)\nnetintel-ocr --diagnostic-report &gt; report.txt\n\n# Include system info\necho \"=== System Info ===\" &gt;&gt; report.txt\nuname -a &gt;&gt; report.txt\npython --version &gt;&gt; report.txt\nlsb_release -a 2&gt;/dev/null &gt;&gt; report.txt\n</code></pre>"},{"location":"troubleshooting/#community-support","title":"Community Support","text":"<ul> <li>GitHub Issues: https://github.com/VisionMLNet/NetIntelOCR/issues</li> <li>PyPI Package: https://pypi.org/project/netintel-ocr/</li> <li>Documentation: https://visionml.net/docs</li> <li>Discord: https://discord.gg/netintel-ocr</li> </ul>"},{"location":"troubleshooting/#include-in-bug-reports","title":"Include in Bug Reports","text":"<ol> <li>Operating System: <code>uname -a</code> (MUST be Linux)</li> <li>Python version: <code>python --version</code> (MUST be 3.11.x or 3.12.x)</li> <li>NetIntel-OCR version: <code>netintel-ocr --version</code></li> <li>Linux distribution: <code>cat /etc/os-release</code></li> <li>Ollama models: <code>ollama list</code></li> <li>Error messages and logs</li> <li>Installation method (pip, Docker, source)</li> <li>Sample PDF (if possible)</li> </ol>"},{"location":"troubleshooting/#next-steps","title":"Next Steps","text":"<ul> <li>Multi-Model Guide - Optimize model selection</li> <li>Performance Guide - Improve processing speed</li> <li>API Integration - Programmatic troubleshooting</li> </ul>"},{"location":"vector-search/","title":"Vector Search Guide","text":""},{"location":"vector-search/#overview","title":"Overview","text":"<p>NetIntel-OCR integrates with Milvus to enable semantic search across processed documents, network components, and architectural patterns.</p>"},{"location":"vector-search/#milvus-setup","title":"Milvus Setup","text":""},{"location":"vector-search/#docker-installation","title":"Docker Installation","text":"<pre><code># Start Milvus standalone\ndocker run -d \\\n  --name milvus-standalone \\\n  -p 19530:19530 \\\n  -p 9091:9091 \\\n  -v milvus_data:/var/lib/milvus \\\n  milvusdb/milvus:latest \\\n  milvus run standalone\n</code></pre>"},{"location":"vector-search/#kubernetes-installation","title":"Kubernetes Installation","text":"<pre><code># Install with Helm\nhelm repo add milvus https://milvus-io.github.io/milvus-helm/\nhelm install milvus milvus/milvus --set cluster.enabled=false\n</code></pre>"},{"location":"vector-search/#document-ingestion","title":"Document Ingestion","text":""},{"location":"vector-search/#basic-ingestion","title":"Basic Ingestion","text":"<pre><code># Process and ingest document\nnetintel-ocr --vector-store milvus \\\n             --milvus-host localhost:19530 \\\n             --ingest \\\n             document.pdf\n</code></pre>"},{"location":"vector-search/#batch-ingestion","title":"Batch Ingestion","text":"<pre><code># Ingest multiple documents\nfor pdf in *.pdf; do\n  netintel-ocr --ingest --collection network-docs \"$pdf\"\ndone\n\n# Or use batch mode\nnetintel-ocr --batch-ingest --collection network-docs *.pdf\n</code></pre>"},{"location":"vector-search/#collection-configuration","title":"Collection Configuration","text":"<pre><code># Define collection schema\nfrom netintel_ocr.vector import create_collection\n\ncreate_collection(\n    name=\"network_components\",\n    dim=768,  # Embedding dimension\n    fields={\n        \"component_type\": \"VARCHAR\",\n        \"security_zone\": \"VARCHAR\", \n        \"criticality\": \"INT32\",\n        \"document_id\": \"VARCHAR\",\n        \"page_number\": \"INT32\"\n    }\n)\n</code></pre>"},{"location":"vector-search/#embedding-generation","title":"Embedding Generation","text":""},{"location":"vector-search/#component-embeddings","title":"Component Embeddings","text":"<p>Documents are processed into multiple embedding types:</p> <pre><code># Generated embeddings\nembeddings = {\n    \"document\": [...],      # Full document embedding\n    \"pages\": [...],        # Per-page embeddings\n    \"components\": [...],   # Network component embeddings\n    \"relationships\": [...] # Connection embeddings\n}\n</code></pre>"},{"location":"vector-search/#custom-embedding-model","title":"Custom Embedding Model","text":"<pre><code># Use specific embedding model\nnetintel-ocr --embedding-model all-MiniLM-L6-v2 \\\n             --ingest document.pdf\n\n# Or use Ollama embeddings\nnetintel-ocr --embedding-model ollama/nomic-embed-text \\\n             --ingest document.pdf\n</code></pre>"},{"location":"vector-search/#search-queries","title":"Search Queries","text":""},{"location":"vector-search/#cli-search","title":"CLI Search","text":"<pre><code># Search for components\nnetintel-ocr --search \"firewall in DMZ zone\"\n\n# Search with filters\nnetintel-ocr --search \"router\" \\\n             --filter \"security_zone=external\" \\\n             --limit 10\n</code></pre>"},{"location":"vector-search/#python-api","title":"Python API","text":"<pre><code>from netintel_ocr.vector import VectorSearch\n\nsearch = VectorSearch(host=\"localhost:19530\")\n\n# Semantic search\nresults = search.query(\n    text=\"Find all components with internet exposure\",\n    collection=\"network_components\",\n    limit=20\n)\n\n# Filtered search\nresults = search.query(\n    text=\"database servers\",\n    filter=\"criticality &gt;= 8\",\n    limit=10\n)\n\n# Component relationship search\nconnections = search.find_connections(\n    source=\"web_server\",\n    max_hops=3\n)\n</code></pre>"},{"location":"vector-search/#rest-api","title":"REST API","text":"<pre><code># Search endpoint\ncurl -X POST http://localhost:8000/search \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"query\": \"firewall configurations\",\n    \"collection\": \"network_docs\",\n    \"limit\": 10,\n    \"filter\": {\n      \"document_type\": \"security\"\n    }\n  }'\n</code></pre>"},{"location":"vector-search/#query-examples","title":"Query Examples","text":""},{"location":"vector-search/#find-similar-network-architectures","title":"Find Similar Network Architectures","text":"<pre><code># Upload reference architecture\nreference = process_diagram(\"reference_architecture.png\")\n\n# Find similar architectures\nsimilar = search.find_similar(\n    embedding=reference.embedding,\n    threshold=0.85,\n    limit=5\n)\n</code></pre>"},{"location":"vector-search/#security-zone-analysis","title":"Security Zone Analysis","text":"<pre><code># Find all components in DMZ\ndmz_components = search.query(\n    filter=\"security_zone='DMZ'\",\n    include_metadata=True\n)\n\n# Find cross-zone connections\ncross_zone = search.query(\n    text=\"connections between internal and external zones\",\n    include_relationships=True\n)\n</code></pre>"},{"location":"vector-search/#compliance-queries","title":"Compliance Queries","text":"<pre><code># Find exposed services\nexposed = search.query(\n    text=\"services accessible from internet\",\n    filter=\"exposure='external'\"\n)\n\n# Find unencrypted connections\nunencrypted = search.query(\n    text=\"connections without encryption\",\n    filter=\"encrypted=false\"\n)\n</code></pre>"},{"location":"vector-search/#index-management","title":"Index Management","text":""},{"location":"vector-search/#create-indexes","title":"Create Indexes","text":"<pre><code>from netintel_ocr.vector import IndexManager\n\nindex = IndexManager()\n\n# Create IVF index for large collections\nindex.create_index(\n    collection=\"network_components\",\n    index_type=\"IVF_FLAT\",\n    metric_type=\"L2\",\n    params={\"nlist\": 1024}\n)\n\n# Create HNSW index for high accuracy\nindex.create_index(\n    collection=\"critical_components\",\n    index_type=\"HNSW\",\n    params={\"M\": 16, \"efConstruction\": 200}\n)\n</code></pre>"},{"location":"vector-search/#optimize-performance","title":"Optimize Performance","text":"<pre><code># Compact collection\nnetintel-ocr --compact-collection network_components\n\n# Build index\nnetintel-ocr --build-index network_components\n\n# Load collection to memory\nnetintel-ocr --load-collection network_components\n</code></pre>"},{"location":"vector-search/#integration-examples","title":"Integration Examples","text":""},{"location":"vector-search/#cmdb-population","title":"CMDB Population","text":"<pre><code>from netintel_ocr import DocumentProcessor\nfrom cmdb_client import CMDBClient\n\n# Process document\nprocessor = DocumentProcessor()\nresults = processor.process(\"network_design.pdf\")\n\n# Populate CMDB\ncmdb = CMDBClient()\nfor component in results.components:\n    cmdb.create_ci({\n        \"name\": component.name,\n        \"type\": component.type,\n        \"attributes\": component.metadata,\n        \"relationships\": component.connections\n    })\n</code></pre>"},{"location":"vector-search/#change-detection","title":"Change Detection","text":"<pre><code># Compare document versions\nold_doc = search.get_document(\"design_v1.pdf\")\nnew_doc = process_document(\"design_v2.pdf\")\n\n# Find changes\nchanges = search.compare_architectures(\n    old_doc.embedding,\n    new_doc.embedding\n)\n\nprint(f\"Added components: {changes.added}\")\nprint(f\"Removed components: {changes.removed}\")\nprint(f\"Modified connections: {changes.modified}\")\n</code></pre>"},{"location":"vector-search/#knowledge-graph","title":"Knowledge Graph","text":"<pre><code># Build knowledge graph\nfrom netintel_ocr.graph import KnowledgeGraph\n\nkg = KnowledgeGraph()\n\n# Add documents to graph\nfor doc in documents:\n    kg.add_document(doc)\n\n# Query relationships\npaths = kg.find_path(\n    from_component=\"internet_gateway\",\n    to_component=\"database_server\"\n)\n\n# Find critical paths\ncritical = kg.find_critical_paths(\n    metric=\"security_exposure\"\n)\n</code></pre>"},{"location":"vector-search/#monitoring","title":"Monitoring","text":""},{"location":"vector-search/#collection-statistics","title":"Collection Statistics","text":"<pre><code># View collection stats\nnetintel-ocr --collection-stats network_components\n\n# Monitor query performance\nnetintel-ocr --query-stats --last 24h\n</code></pre>"},{"location":"vector-search/#health-checks","title":"Health Checks","text":"<pre><code># Check Milvus health\nfrom netintel_ocr.vector import health_check\n\nstatus = health_check()\nprint(f\"Milvus status: {status.state}\")\nprint(f\"Collections: {status.collections}\")\nprint(f\"Total vectors: {status.total_vectors}\")\n</code></pre>"},{"location":"vector-search/#best-practices","title":"Best Practices","text":"<ol> <li>Collection Partitioning: Partition by document type or date</li> <li>Embedding Cache: Cache frequently accessed embeddings</li> <li>Batch Operations: Use batch insert for large datasets</li> <li>Index Selection: Choose index based on dataset size</li> <li>Regular Compaction: Compact collections weekly</li> </ol>"},{"location":"vector-search/#next-steps","title":"Next Steps","text":"<ul> <li>Customization Guide - Tune extraction and embeddings</li> <li>Deployment Guide - Production Milvus setup</li> <li>Quick Start Guide - Basic usage examples</li> </ul>"}]}